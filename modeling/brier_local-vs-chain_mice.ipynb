{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Relative Brier Score - Local Models vs Model Chains (using MICE on the whole dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compares the average relative brier scores of the local models and the two model chains (where we propagate the true values of the predictions) for the case where we impute the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error as mse, brier_score_loss\n",
    "from chaining import Chain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingness_stratified_cv(df, N_FOLDS=5, random_state=None):\n",
    "    # Add seed for reproducibility of the predictions (to get the same scores each time we run the code)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Initial complete-case test fold assignment\n",
    "    cv = pd.Series(np.nan, index=df.index)\n",
    "    i_cc = (df.isna().sum(axis=1) == 0) # Complete cases\n",
    "    cv.iloc[i_cc] = np.random.randint(low=0, high=N_FOLDS, size=i_cc.sum())\n",
    "\n",
    "    # Go over columns from most missing to least missing\n",
    "    for j in df.isna().sum().argsort()[::-1]:\n",
    "        # Instances i that are not assigned yet but for which df[i,j] is observed\n",
    "        i_tbf = (cv.isna()) & (~df.iloc[:,j].isna()) # to be filled\n",
    "        # Fill them randomly\n",
    "        cv.iloc[i_tbf] = np.random.randint(low=0, high=N_FOLDS, size=i_tbf.sum())\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingness_and_categorical_stratified_cv(df, N_FOLDS=5, random_state=None):\n",
    "    # Add seed for reproducibility of the predictions (to get the same scores each time we run the code)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Initial complete-case test fold assignment\n",
    "    cv = pd.Series(np.nan, index=df.index)\n",
    "    i_cc = (df.isna().sum(axis=1) == 0) # Complete cases\n",
    "    cv.iloc[i_cc] = np.random.randint(low=0, high=N_FOLDS, size=i_cc.sum())\n",
    "\n",
    "    # Stratify categorical variables\n",
    "    for col in df.select_dtypes(include=['category']):\n",
    "        counts = df[col].value_counts(normalize=True)\n",
    "        for category in counts.index:\n",
    "            idx = df[col] == category\n",
    "            cv[idx] = cv[idx].fillna(np.random.choice(np.where(idx)[0], size=int(counts[category] * N_FOLDS), replace=False))\n",
    "\n",
    "    # Go over columns from most missing to least missing\n",
    "    for j in df.isna().sum().argsort()[::-1]:\n",
    "        # Instances i that are not assigned yet but for which df[i,j] is observed\n",
    "        i_tbf = (cv.isna()) & (~df.iloc[:,j].isna()) # to be filled\n",
    "        # Fill them randomly\n",
    "        cv.iloc[i_tbf] = np.random.randint(low=0, high=N_FOLDS, size=i_tbf.sum())\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert path to data file here\n",
    "possible_paths = [\n",
    "    'C:/Users/lenne/OneDrive/Documenten/Master of Statistics and Data Science/2023-2024/Master thesis/Thesis_Sofia_Lennert/new_data',\n",
    "    'C:/Users/anaso/Desktop/SOFIA MENDES/KU Leuven/Master Thesis/Thesis_Sofia_Lennert/new_data'\n",
    "]\n",
    "\n",
    "# File name\n",
    "file = 'merged_data.csv'\n",
    "\n",
    "# Find full paths to the CSV files\n",
    "path = next((f'{path}/{file}' for path in possible_paths if os.path.exists(f'{path}/{file}')), None)\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Bin the number of relapses into 0, 1, 2, 3 and 4+ \n",
    "def bin_column(value):\n",
    "    if value in [0, 1, 2, 3]:\n",
    "        return str(value)\n",
    "    else:\n",
    "        return '4+'\n",
    "data['NRELAP'] = data['NRELAP'].apply(bin_column)\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice of target variables, and listed already in the chain order \n",
    "variables = ['KFSS_M-2y', 'KFSS_P-2y', 'EDSS-2y', 'T25FW-2y', 'NHPT-2y', 'P_R36-SF12-after', 'M_R36-SF12-after', \n",
    "             'SES_after', 'SLEC_after', 'KFSS_M-after_2y', 'KFSS_P-after_2y', 'EDSS-after_2y', 'NRELAP', 'CESEV']\n",
    "\n",
    "# Extract targets\n",
    "targets = data[variables]\n",
    "\n",
    "# Choice of input variables\n",
    "columns_to_keep = ['AGE', 'SEX', 'RACE', 'CONTINENT', 'MHDIAGN', 'CARDIO', 'URINARY', 'MUSCKELET', 'FATIGUE', \n",
    "                    'NHPT-before', 'PASAT_2s-before', 'PASAT_3s-before', 'SDMT-before', 'T25FW-before', 'SLEC_before','SES_before',\n",
    "                    'BDI-before', 'EDSS-before', 'KFSS_M-before', 'KFSS_P-before', 'M_R36-SF12-before',\n",
    "                \t'P_R36-SF12-before', 'R36-SF12-before_Ind', 'T-before','P-before','N-before']\n",
    "\n",
    "# Extract features\n",
    "features = data[columns_to_keep]\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one-hot encoding for categorical and binary input variables\n",
    "object_columns = features.select_dtypes(include=['object'])\n",
    "features = pd.get_dummies(features, columns=object_columns.columns, dtype=int)\n",
    "#features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFSS_M-2y           float64\n",
       "KFSS_P-2y           float64\n",
       "EDSS-2y             float64\n",
       "T25FW-2y            float64\n",
       "NHPT-2y             float64\n",
       "P_R36-SF12-after    float64\n",
       "M_R36-SF12-after    float64\n",
       "SES_after           float64\n",
       "SLEC_after          float64\n",
       "KFSS_M-after_2y     float64\n",
       "KFSS_P-after_2y     float64\n",
       "EDSS-after_2y       float64\n",
       "NRELAP               object\n",
       "CESEV                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MICE (on the entire dataset, using a two-step approach: first on X, then on [X_imp, Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenne\\anaconda3\\envs\\Thesis\\lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: impute the features\n",
    "featuresM=features.copy()\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "imputed_values = imputer.fit_transform(featuresM)\n",
    "\n",
    "featuresM = pd.DataFrame(imputed_values, columns=featuresM.columns)\n",
    "#featuresM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of values for each numerical column:\n",
      "AGE                     65.055452\n",
      "NHPT-before            288.400000\n",
      "PASAT_2s-before         67.470180\n",
      "PASAT_3s-before         58.500000\n",
      "SDMT-before            599.509092\n",
      "T25FW-before           131.400000\n",
      "SLEC_before            676.739216\n",
      "SES_before               1.195100\n",
      "BDI-before               0.856293\n",
      "EDSS-before              6.500000\n",
      "KFSS_M-before            0.685185\n",
      "KFSS_P-before            0.750000\n",
      "M_R36-SF12-before        0.885714\n",
      "P_R36-SF12-before        0.769231\n",
      "R36-SF12-before_Ind      1.292154\n",
      "T-before                 1.004456\n",
      "P-before                 1.001466\n",
      "N-before                 1.009040\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Compute the range for the numerical columns\n",
    "ranges = featuresM.apply(lambda x: x.max() - x.min())\n",
    "filtered_ranges = ranges[ranges != 1]\n",
    "\n",
    "print(\"Range of values for each numerical column:\")\n",
    "print(filtered_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate imputed features and targets\n",
    "model_data = pd.concat([featuresM, targets], axis=1)\n",
    "#model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenne\\anaconda3\\envs\\Thesis\\lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Impute the targets, based on model_data (= imputed features + targets)\n",
    "le1 = LabelEncoder()\n",
    "le2 = LabelEncoder()\n",
    "cesev = le1.fit_transform(np.array(model_data['CESEV']))\n",
    "nrelap = le2.fit_transform(np.array(model_data['NRELAP']))\n",
    "\n",
    "model_data['CESEV'] = cesev\n",
    "model_data[\"CESEV\"] = model_data[\"CESEV\"].replace(3, np.nan)\n",
    "model_data['NRELAP'] = nrelap\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "imputed_values = imputer.fit_transform(model_data)\n",
    "\n",
    "# Convert imputed values back to DataFrame\n",
    "encoded_data = pd.DataFrame(imputed_values, columns=model_data.columns)\n",
    "#encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_data[(encoded_data[\"CESEV\"] <= -0.5) | (encoded_data[\"CESEV\"] >= 2.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data.loc[encoded_data['CESEV'] < -0.5, 'CESEV'] = 0\n",
    "\n",
    "cesev = np.array(encoded_data['CESEV']).round().astype(int)\n",
    "nrelap = np.array(encoded_data['NRELAP']).round().astype(int)\n",
    "\n",
    "def replace_three(arr):\n",
    "    return np.where(arr == 3, 2, arr)\n",
    "cesev = replace_three(cesev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MODERATE', 'MILD', 'SEVERE'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data['CESEV'] = le1.inverse_transform(cesev)\n",
    "encoded_data['NRELAP'] = le2.inverse_transform(nrelap)\n",
    "encoded_data['CESEV'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetsM = encoded_data[targets.columns]\n",
    "#targetsM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of values for each numerical column:\n",
      "KFSS_M-2y             0.759259\n",
      "KFSS_P-2y             0.750000\n",
      "EDSS-2y               8.000000\n",
      "T25FW-2y            177.408688\n",
      "NHPT-2y             289.100000\n",
      "P_R36-SF12-after      0.750000\n",
      "M_R36-SF12-after      1.000000\n",
      "SES_after             1.885904\n",
      "SLEC_after          468.823191\n",
      "KFSS_M-after_2y       0.677183\n",
      "KFSS_P-after_2y       0.866265\n",
      "EDSS-after_2y        11.633106\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Compute the range for the numerical targets (all except last 2)\n",
    "selected_columns = targetsM.iloc[:, :-2]\n",
    "ranges = selected_columns.apply(lambda x: x.max() - x.min())\n",
    "print(\"Range of values for each numerical column:\")\n",
    "print(ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random state for reproducibility\n",
    "random_state = 42\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV Fold\n",
       "4.0    510\n",
       "3.0    502\n",
       "0.0    500\n",
       "1.0    495\n",
       "2.0    458\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate CV folds\n",
    "cv=missingness_and_categorical_stratified_cv(targets, N_FOLDS, random_state)\n",
    "cv = cv.to_frame(name=\"CV Fold\")\n",
    "\n",
    "featuresM_cv = pd.merge(featuresM, pd.DataFrame(cv), left_index=True, right_index=True)\n",
    "targetsM_cv = pd.merge(targetsM, pd.DataFrame(cv), left_index=True, right_index=True)\n",
    "targets_cv = pd.merge(targets, pd.DataFrame(cv), left_index=True, right_index=True)\n",
    "\n",
    "featuresM_cv['CV Fold'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina a function to calculate the normalized MSE\n",
    "def normalized_mean_squared_error(true, pred, train):\n",
    "    num = mse(true, pred)\n",
    "    mean_value = np.mean(train)\n",
    "    mean = np.full_like(true, mean_value)\n",
    "    den = mse(true, mean)\n",
    "    nmse_loss = num/den\n",
    "    return nmse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with evaluating on CV Fold 1\n",
      "Done with evaluating on CV Fold 2\n",
      "Done with evaluating on CV Fold 3\n",
      "Done with evaluating on CV Fold 4\n",
      "Done with evaluating on CV Fold 5\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "y_train_list = []\n",
    "y_pred_prob_list = []\n",
    "yi_test_dummies_list = []\n",
    "yi_train_dummies_list = []\n",
    "\n",
    "\n",
    "for i in range(0, N_FOLDS): \n",
    "    Xi_train = featuresM_cv[featuresM_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    Xi_test = featuresM_cv[featuresM_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_train = targetsM_cv[targetsM_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_test = targets_cv[targets_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    y_test_list.append(pd.DataFrame(yi_test, columns=yi_test.columns, index=yi_test.index))\n",
    "    y_train_list.append(pd.DataFrame(yi_train, columns=yi_train.columns, index=yi_train.index))\n",
    "\n",
    "    # One hot encode categorical targets of test set to be able to compute brier score\n",
    "    subset_yi_test = yi_test.select_dtypes(include=['object'])\n",
    "    yi_test_dummies = pd.get_dummies(subset_yi_test, columns=subset_yi_test.columns, dtype=int)\n",
    "    subset_yi_train = yi_train.select_dtypes(include=['object'])\n",
    "    yi_train_dummies = pd.get_dummies(subset_yi_train, columns=subset_yi_train.columns, dtype=int)\n",
    "    \n",
    "\n",
    "    chain = Chain(\n",
    "        model_reg=RandomForestRegressor(random_state=random_state),\n",
    "        model_clf=RandomForestClassifier(random_state=random_state),\n",
    "        propagate=False, #RUN LOCAL MODELS \n",
    "    )\n",
    "\n",
    "\n",
    "    chain.fit(Xi_train, yi_train, target_types=None) #[\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"clf\",\"clf\"]\n",
    "    y_pred = chain.predict(Xi_test)\n",
    "    y_pred_prob = chain.predict_proba(Xi_test)\n",
    "    y_pred_list.append(y_pred)\n",
    "    y_pred_prob_list.append(y_pred_prob)\n",
    "    yi_test_dummies_list.append(yi_test_dummies)\n",
    "    yi_train_dummies_list.append(yi_train_dummies)\n",
    "    \n",
    "    print(\"Done with evaluating on CV Fold {}\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain values of baseline model\n",
    "yi_train_dummies_avg = []\n",
    "i=0\n",
    " \n",
    "for yi_train_dummies_fold in yi_train_dummies_list:\n",
    "\n",
    "    percentages = yi_train_dummies_fold.sum() / len(yi_train_dummies_fold)\n",
    "\n",
    "    yi_train_dummies_avg_fold = pd.DataFrame(0, index=yi_test_dummies_list[i].index, columns=yi_train_dummies_fold.columns)\n",
    "\n",
    "    # Replace values in each column with the corresponding percentage\n",
    "    for col in yi_train_dummies_avg_fold.columns:\n",
    "        yi_train_dummies_avg_fold[col] = yi_train_dummies_avg_fold[col].apply(lambda x: percentages[col])\n",
    "    \n",
    "    i += 1\n",
    "    yi_train_dummies_avg.append(yi_train_dummies_avg_fold)\n",
    "\n",
    "\n",
    "\n",
    "# Re-arrange output of predicted probabilities\n",
    "concatenated_dfs = []\n",
    "\n",
    "# Iterate over each pair of arrays\n",
    "for j, fold in enumerate(y_pred_prob_list):\n",
    "    dfs = []\n",
    "    len_array = 0\n",
    "    \n",
    "    for i, array in enumerate(fold):\n",
    "        col = yi_test_dummies_list[j].columns[len_array:len_array+len(array[0])]\n",
    "        df = pd.DataFrame(array, columns=col, index=yi_test_dummies_list[j].index)\n",
    "        dfs.append(df)\n",
    "        len_array += len(array[0])\n",
    "    \n",
    "    concatenated_df = pd.concat(dfs, axis=1)\n",
    "    concatenated_dfs.append(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Brier scores for each categorical variable:\n",
      "NRELAP: 0.97 \n",
      "CESEV: 1.12 \n"
     ]
    }
   ],
   "source": [
    "scores_with_std = []\n",
    "avg_brier_score = []\n",
    "avg_baseline_score = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "cat_normalized_brier=[]\n",
    "\n",
    "# Create a dictionary to store the scores for variables with the same letters before the '_'\n",
    "brier_scores_dict = {}\n",
    "baseline_scores_dict = {}\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for level_name in variables_cat: \n",
    "    brier_scores = []\n",
    "    baseline_scores = []\n",
    "    \n",
    "    # Scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][level_name] \n",
    "        y_prob = concatenated_dfs[fold_index][level_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][level_name] \n",
    "        \n",
    "        # Compute the Brier score and the baseline Brier score\n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        N_brier_score = brier_score\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "        N_brier_baseline = brier_baseline\n",
    "\n",
    "        brier_scores.append(N_brier_score)\n",
    "        baseline_scores.append(N_brier_baseline)\n",
    "    \n",
    "    # Check if the variable name has letters before the '_'\n",
    "    prefix = level_name.split('_')[0]\n",
    "    \n",
    "    # Add the Brier scores to the dictionary based on the prefix\n",
    "    if prefix in brier_scores_dict:\n",
    "        brier_scores_dict[prefix].extend(brier_scores)\n",
    "    else:\n",
    "        brier_scores_dict[prefix] = brier_scores\n",
    "\n",
    "    if prefix in baseline_scores_dict:\n",
    "        baseline_scores_dict[prefix].extend(baseline_scores)\n",
    "    else:\n",
    "        baseline_scores_dict[prefix] = baseline_scores\n",
    "\n",
    "# Average of normalized Brier score for each prefix\n",
    "for prefix, scores in brier_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_brier_score.append((prefix, sum_score))\n",
    "\n",
    "for prefix, scores in baseline_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_baseline_score.append((prefix, sum_score))\n",
    "\n",
    "normalized_score_list = []\n",
    "for i in range(len(avg_brier_score)):\n",
    "    normalized_score = avg_brier_score[i][1]/avg_baseline_score[i][1]\n",
    "    cell = (avg_brier_score[i][0], normalized_score)\n",
    "    normalized_score_list.append(cell)\n",
    "\n",
    "\n",
    "print(\"Normalized Brier scores for each categorical variable:\")\n",
    "for prefix, avg_score in normalized_score_list:\n",
    "    print(f\"{prefix}: {avg_score:.2f} \")\n",
    "    cat_normalized_brier.append(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for each outcome (local):\n",
      "KFSS_M-2y: 0.18 (± 0.02)\n",
      "KFSS_P-2y: 0.25 (± 0.04)\n",
      "EDSS-2y: 0.12 (± 0.01)\n",
      "T25FW-2y: 0.27 (± 0.08)\n",
      "NHPT-2y: 0.53 (± 0.13)\n",
      "P_R36-SF12-after: 0.31 (± 0.02)\n",
      "M_R36-SF12-after: 0.43 (± 0.03)\n",
      "SES_after: 0.27 (± 0.06)\n",
      "SLEC_after: 0.32 (± 0.03)\n",
      "KFSS_M-after_2y: 0.31 (± 0.02)\n",
      "KFSS_P-after_2y: 0.44 (± 0.04)\n",
      "EDSS-after_2y: 0.24 (± 0.02)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows in y_test and y_pred where the variable in question is missing in y_test (since without it, it is not possible to calculate the score)\n",
    "y_test_cv = []\n",
    "y_pred_cv = []\n",
    "\n",
    "for j in range(len(y_test_list)): \n",
    "    y_test_targ = []\n",
    "    y_pred_targ = []\n",
    "    nvar=y_test_list[0].shape[1]\n",
    "\n",
    "    for i in range(0, nvar):  \n",
    "        missing_rows_mask = y_test_list[j].iloc[:, i].isna()\n",
    "        y_test = y_test_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        y_pred = y_pred_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        \n",
    "        y_test_targ.append(y_test)\n",
    "        y_pred_targ.append(y_pred)\n",
    "    \n",
    "    y_test_cv.append(y_test_targ)\n",
    "    y_pred_cv.append(y_pred_targ)\n",
    "\n",
    "\n",
    "\n",
    "# OBTAIN NORMALIZED MSE \n",
    "\n",
    "scores_with_std = []\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for variable_name in variables: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Check if the target variable is numerical or categorical\n",
    "    if y_test_cv[0][variables.index(variable_name)].dtype.kind in 'bifc':\n",
    "        # Scores for the variable across all folds\n",
    "        for fold_index in range(len(y_test_cv)):\n",
    "            y_test = y_test_cv[fold_index][variables.index(variable_name)] \n",
    "            y_pred = y_pred_cv[fold_index][variables.index(variable_name)] \n",
    "            y_train = y_train_list[fold_index][variable_name]\n",
    "\n",
    "            score = normalized_mean_squared_error(y_test, y_pred, y_train)\n",
    "            variable_scores.append(score)\n",
    "        \n",
    "        # Average score for the variable across all folds\n",
    "        variable_avg_score = np.mean(variable_scores)\n",
    "        \n",
    "        # Standard deviation for the variable across all folds\n",
    "        variable_std_score = np.std(variable_scores)\n",
    "        \n",
    "        # Append the tuple with three elements to the scores_with_std list\n",
    "        scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "num_normalized_brier=[]\n",
    "num_std_brier=[]\n",
    "\n",
    "print(\"Scores for each outcome (local):\")\n",
    "for variable_name, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    num_normalized_brier.append(avg_score)\n",
    "    num_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18041131 0.24729066 0.11577412 0.27091814 0.53432337 0.30892812\n",
      " 0.42600152 0.27338941 0.32242882 0.31185565 0.44183062 0.23871444\n",
      " 0.96835378 1.1181914 ]\n",
      "Average relative Brier score: 0.46595037702586134\n"
     ]
    }
   ],
   "source": [
    "# Concatenate normalized brier scores for all variables (both numerical and categorical) \n",
    "combined_normalized_brier = np.concatenate((num_normalized_brier, cat_normalized_brier))\n",
    "print(combined_normalized_brier)\n",
    "\n",
    "# Average relative Brier score\n",
    "average_normalized_brier = np.mean(combined_normalized_brier)\n",
    "print(\"Average relative Brier score:\", average_normalized_brier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propagate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with evaluating on CV Fold 1\n",
      "Done with evaluating on CV Fold 2\n",
      "Done with evaluating on CV Fold 3\n",
      "Done with evaluating on CV Fold 4\n",
      "Done with evaluating on CV Fold 5\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "y_train_list = []\n",
    "y_pred_prob_list = []\n",
    "yi_test_dummies_list = []\n",
    "yi_train_dummies_list = []\n",
    "\n",
    "for i in range(0, N_FOLDS): \n",
    "    Xi_train = featuresM_cv[featuresM_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    Xi_test = featuresM_cv[featuresM_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_train = targetsM_cv[targetsM_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_test = targets_cv[targets_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    y_test_list.append(pd.DataFrame(yi_test, columns=yi_test.columns, index=yi_test.index))\n",
    "    y_train_list.append(pd.DataFrame(yi_train, columns=yi_train.columns, index=yi_train.index))\n",
    "\n",
    "    # One hot encode categorical targets of test set to be able to compute brier score\n",
    "    subset_yi_test = yi_test.select_dtypes(include=['object'])\n",
    "    yi_test_dummies = pd.get_dummies(subset_yi_test, columns=subset_yi_test.columns, dtype=int)\n",
    "    subset_yi_train = yi_train.select_dtypes(include=['object'])\n",
    "    yi_train_dummies = pd.get_dummies(subset_yi_train, columns=subset_yi_train.columns, dtype=int)\n",
    "    \n",
    "\n",
    "    chain = Chain(\n",
    "        model_reg=RandomForestRegressor(random_state=random_state),\n",
    "        model_clf=RandomForestClassifier(random_state=random_state),\n",
    "        propagate=\"pred\", \n",
    "    )\n",
    "\n",
    "\n",
    "    chain.fit(Xi_train, yi_train, target_types=None) #[\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"clf\",\"clf\"]\n",
    "    y_pred = chain.predict(Xi_test)\n",
    "    y_pred_prob = chain.predict_proba(Xi_test)\n",
    "    y_pred_list.append(y_pred)\n",
    "    y_pred_prob_list.append(y_pred_prob)\n",
    "    yi_test_dummies_list.append(yi_test_dummies)\n",
    "    yi_train_dummies_list.append(yi_train_dummies)\n",
    "    \n",
    "    print(\"Done with evaluating on CV Fold {}\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain values of baseline model\n",
    "yi_train_dummies_avg = []\n",
    "i=0\n",
    " \n",
    "for yi_train_dummies_fold in yi_train_dummies_list:\n",
    "\n",
    "    percentages = yi_train_dummies_fold.sum() / len(yi_train_dummies_fold)\n",
    "\n",
    "    yi_train_dummies_avg_fold = pd.DataFrame(0, index=yi_test_dummies_list[i].index, columns=yi_train_dummies_fold.columns)\n",
    "\n",
    "    # Replace values in each column with the corresponding percentage\n",
    "    for col in yi_train_dummies_avg_fold.columns:\n",
    "        yi_train_dummies_avg_fold[col] = yi_train_dummies_avg_fold[col].apply(lambda x: percentages[col])\n",
    "    \n",
    "    i += 1\n",
    "    yi_train_dummies_avg.append(yi_train_dummies_avg_fold)\n",
    "\n",
    "\n",
    "\n",
    "# Re-arrange output of predicted probabilities\n",
    "concatenated_dfs = []\n",
    "\n",
    "# Iterate over each pair of arrays\n",
    "for j, fold in enumerate(y_pred_prob_list):\n",
    "    dfs = []\n",
    "    len_array = 0\n",
    "    \n",
    "    for i, array in enumerate(fold):\n",
    "        col = yi_test_dummies_list[j].columns[len_array:len_array+len(array[0])]\n",
    "        df = pd.DataFrame(array, columns=col, index=yi_test_dummies_list[j].index)\n",
    "        dfs.append(df)\n",
    "        len_array += len(array[0])\n",
    "    \n",
    "    concatenated_df = pd.concat(dfs, axis=1)\n",
    "    concatenated_dfs.append(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Brier scores for each categorical variable:\n",
      "NRELAP: 0.97 \n",
      "CESEV: 1.14 \n"
     ]
    }
   ],
   "source": [
    "scores_with_std = []\n",
    "avg_brier_score = []\n",
    "avg_baseline_score = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "cat_normalized_brier=[]\n",
    "\n",
    "# Create a dictionary to store the scores for variables with the same letters before the '_'\n",
    "brier_scores_dict = {}\n",
    "baseline_scores_dict = {}\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for level_name in variables_cat: \n",
    "    brier_scores = []\n",
    "    baseline_scores = []\n",
    "    \n",
    "    # Scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][level_name] \n",
    "        y_prob = concatenated_dfs[fold_index][level_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][level_name] \n",
    "        \n",
    "        # Compute the Brier score and baseline brier score\n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        N_brier_score = brier_score\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "        N_brier_baseline = brier_baseline\n",
    "\n",
    "        # Append the Brier score to the variable scores list\n",
    "        brier_scores.append(N_brier_score)\n",
    "        baseline_scores.append(N_brier_baseline)\n",
    "    \n",
    "    # Check if the variable name has letters before the '_'\n",
    "    prefix = level_name.split('_')[0]\n",
    "    \n",
    "    # Add the Brier scores to the dictionary based on the prefix\n",
    "    if prefix in brier_scores_dict:\n",
    "        brier_scores_dict[prefix].extend(brier_scores)\n",
    "    else:\n",
    "        brier_scores_dict[prefix] = brier_scores\n",
    "\n",
    "    if prefix in baseline_scores_dict:\n",
    "        baseline_scores_dict[prefix].extend(baseline_scores)\n",
    "    else:\n",
    "        baseline_scores_dict[prefix] = baseline_scores\n",
    "\n",
    "# Compute the average of Brier score for each prefix\n",
    "for prefix, scores in brier_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_brier_score.append((prefix, sum_score))\n",
    "\n",
    "for prefix, scores in baseline_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_baseline_score.append((prefix, sum_score))\n",
    "\n",
    "normalized_score_list = []\n",
    "for i in range(len(avg_brier_score)):\n",
    "    normalized_score = avg_brier_score[i][1]/avg_baseline_score[i][1]\n",
    "    cell = (avg_brier_score[i][0], normalized_score)\n",
    "    normalized_score_list.append(cell)\n",
    "\n",
    "\n",
    "print(\"Normalized Brier scores for each categorical variable:\")\n",
    "for prefix, avg_score in normalized_score_list:\n",
    "    print(f\"{prefix}: {avg_score:.2f} \")\n",
    "    cat_normalized_brier.append(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for each outcome (chain - propagate predictions):\n",
      "KFSS_M-2y: 0.18 (± 0.02)\n",
      "KFSS_P-2y: 0.25 (± 0.04)\n",
      "EDSS-2y: 0.12 (± 0.01)\n",
      "T25FW-2y: 0.26 (± 0.07)\n",
      "NHPT-2y: 0.43 (± 0.12)\n",
      "P_R36-SF12-after: 0.30 (± 0.02)\n",
      "M_R36-SF12-after: 0.44 (± 0.04)\n",
      "SES_after: 0.27 (± 0.06)\n",
      "SLEC_after: 0.32 (± 0.03)\n",
      "KFSS_M-after_2y: 0.31 (± 0.03)\n",
      "KFSS_P-after_2y: 0.46 (± 0.05)\n",
      "EDSS-after_2y: 0.26 (± 0.02)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows in y_test and y_pred where the variable in question is missing in y_test (since without it, it is not possible to calculate the score)\n",
    "y_test_cv = []\n",
    "y_pred_cv = []\n",
    "\n",
    "for j in range(len(y_test_list)): \n",
    "    y_test_targ = []\n",
    "    y_pred_targ = []\n",
    "    nvar=y_test_list[0].shape[1]\n",
    "\n",
    "    for i in range(0, nvar):  \n",
    "        missing_rows_mask = y_test_list[j].iloc[:, i].isna()\n",
    "        y_test = y_test_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        y_pred = y_pred_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        \n",
    "        y_test_targ.append(y_test)\n",
    "        y_pred_targ.append(y_pred)\n",
    "    \n",
    "    y_test_cv.append(y_test_targ)\n",
    "    y_pred_cv.append(y_pred_targ)\n",
    "\n",
    "\n",
    "\n",
    "# OBTAIN NORMALIZED MSE \n",
    "\n",
    "scores_with_std = []\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for variable_name in variables: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Check if the target variable is numerical or categorical\n",
    "    if y_test_cv[0][variables.index(variable_name)].dtype.kind in 'bifc':\n",
    "        # Scores for the variable across all folds\n",
    "        for fold_index in range(len(y_test_cv)):\n",
    "            y_test = y_test_cv[fold_index][variables.index(variable_name)] \n",
    "            y_pred = y_pred_cv[fold_index][variables.index(variable_name)] \n",
    "            y_train = y_train_list[fold_index][variable_name]\n",
    "\n",
    "            score = normalized_mean_squared_error(y_test, y_pred, y_train)\n",
    "            variable_scores.append(score)\n",
    "        \n",
    "        # Average score for the variable across all folds\n",
    "        variable_avg_score = np.mean(variable_scores)\n",
    "        \n",
    "        # Standard deviation for the variable across all folds\n",
    "        variable_std_score = np.std(variable_scores)\n",
    "        \n",
    "        # Append the tuple with three elements to the scores_with_std list\n",
    "        scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "num_normalized_brier=[]\n",
    "num_std_brier=[]\n",
    "\n",
    "print(\"Scores for each outcome (chain - propagate predictions):\")\n",
    "for variable_name, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    num_normalized_brier.append(avg_score)\n",
    "    num_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18041131 0.24995976 0.1196163  0.26260525 0.43308402 0.30326788\n",
      " 0.44140606 0.27202907 0.3219111  0.311733   0.45616224 0.25727594\n",
      " 0.97425698 1.1401794 ]\n",
      "Average relative Brier score: 0.4667296671428138\n"
     ]
    }
   ],
   "source": [
    "# Concatenate normalized brier scores for all variables (both numerical and categorical) \n",
    "combined_normalized_brier = np.concatenate((num_normalized_brier, cat_normalized_brier))\n",
    "print(combined_normalized_brier)\n",
    "\n",
    "# Average relative Brier score\n",
    "average_normalized_brier = np.mean(combined_normalized_brier)\n",
    "print(\"Average relative Brier score:\", average_normalized_brier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propagate true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with evaluating on CV Fold 1\n",
      "Done with evaluating on CV Fold 2\n",
      "Done with evaluating on CV Fold 3\n",
      "Done with evaluating on CV Fold 4\n",
      "Done with evaluating on CV Fold 5\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "y_train_list = []\n",
    "y_pred_prob_list = []\n",
    "yi_test_dummies_list = []\n",
    "yi_train_dummies_list = []\n",
    "\n",
    "for i in range(0, N_FOLDS): \n",
    "    Xi_train = featuresM_cv[featuresM_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    Xi_test = featuresM_cv[featuresM_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_train = targetsM_cv[targetsM_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_test = targets_cv[targets_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    y_test_list.append(pd.DataFrame(yi_test, columns=yi_test.columns, index=yi_test.index))\n",
    "    y_train_list.append(pd.DataFrame(yi_train, columns=yi_train.columns, index=yi_train.index))\n",
    "\n",
    "\n",
    "    # One hot encode categorical targets of test set to be able to compute brier score\n",
    "    subset_yi_test = yi_test.select_dtypes(include=['object'])\n",
    "    yi_test_dummies = pd.get_dummies(subset_yi_test, columns=subset_yi_test.columns, dtype=int)\n",
    "    subset_yi_train = yi_train.select_dtypes(include=['object'])\n",
    "    yi_train_dummies = pd.get_dummies(subset_yi_train, columns=subset_yi_train.columns, dtype=int)\n",
    "    \n",
    "\n",
    "    chain = Chain(\n",
    "        model_reg=RandomForestRegressor(random_state=random_state),\n",
    "        model_clf=RandomForestClassifier(random_state=random_state),\n",
    "        propagate=\"true\", \n",
    "    )\n",
    "    chain.fit(Xi_train, yi_train, target_types=None) #[\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"clf\",\"clf\"]\n",
    "    y_pred = chain.predict(Xi_test)\n",
    "    y_pred_prob = chain.predict_proba(Xi_test)\n",
    "    y_pred_list.append(y_pred)\n",
    "    y_pred_prob_list.append(y_pred_prob)\n",
    "    yi_test_dummies_list.append(yi_test_dummies)\n",
    "    yi_train_dummies_list.append(yi_train_dummies)\n",
    "    \n",
    "    print(\"Done with evaluating on CV Fold {}\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain values of baseline model\n",
    "yi_train_dummies_avg = []\n",
    "i=0\n",
    " \n",
    "for yi_train_dummies_fold in yi_train_dummies_list:\n",
    "\n",
    "    percentages = yi_train_dummies_fold.sum() / len(yi_train_dummies_fold)\n",
    "\n",
    "    yi_train_dummies_avg_fold = pd.DataFrame(0, index=yi_test_dummies_list[i].index, columns=yi_train_dummies_fold.columns)\n",
    "\n",
    "    # Replace values in each column with the corresponding percentage\n",
    "    for col in yi_train_dummies_avg_fold.columns:\n",
    "        yi_train_dummies_avg_fold[col] = yi_train_dummies_avg_fold[col].apply(lambda x: percentages[col])\n",
    "    \n",
    "    i += 1\n",
    "    yi_train_dummies_avg.append(yi_train_dummies_avg_fold)\n",
    "\n",
    "\n",
    "\n",
    "# Re-arrange output of predicted probabilities\n",
    "concatenated_dfs = []\n",
    "\n",
    "# Iterate over each pair of arrays\n",
    "for j, fold in enumerate(y_pred_prob_list):\n",
    "    dfs = []\n",
    "    len_array = 0\n",
    "    \n",
    "    for i, array in enumerate(fold):\n",
    "        col = yi_test_dummies_list[j].columns[len_array:len_array+len(array[0])]\n",
    "        df = pd.DataFrame(array, columns=col, index=yi_test_dummies_list[j].index)\n",
    "        dfs.append(df)\n",
    "        len_array += len(array[0])\n",
    "    \n",
    "    concatenated_df = pd.concat(dfs, axis=1)\n",
    "    concatenated_dfs.append(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Brier scores for each categorical variable:\n",
      "NRELAP: 1.23 \n",
      "CESEV: 1.32 \n"
     ]
    }
   ],
   "source": [
    "scores_with_std = []\n",
    "avg_brier_score = []\n",
    "avg_baseline_score = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "cat_normalized_brier=[]\n",
    "\n",
    "# Create a dictionary to store the scores for variables with the same letters before the '_'\n",
    "brier_scores_dict = {}\n",
    "baseline_scores_dict = {}\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for level_name in variables_cat: \n",
    "    brier_scores = []\n",
    "    baseline_scores = []\n",
    "    \n",
    "    # Compute scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][level_name] \n",
    "        y_prob = concatenated_dfs[fold_index][level_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][level_name] \n",
    "        \n",
    "        # Compute the Brier score and baseline brier score\n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        N_brier_score = brier_score\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "        N_brier_baseline = brier_baseline\n",
    "\n",
    "        brier_scores.append(N_brier_score)\n",
    "        baseline_scores.append(N_brier_baseline)\n",
    "    \n",
    "    # Check if the variable name has letters before the '_'\n",
    "    prefix = level_name.split('_')[0]\n",
    "    \n",
    "    # Add the Brier scores to the dictionary based on the prefix\n",
    "    if prefix in brier_scores_dict:\n",
    "        brier_scores_dict[prefix].extend(brier_scores)\n",
    "    else:\n",
    "        brier_scores_dict[prefix] = brier_scores\n",
    "\n",
    "    if prefix in baseline_scores_dict:\n",
    "        baseline_scores_dict[prefix].extend(baseline_scores)\n",
    "    else:\n",
    "        baseline_scores_dict[prefix] = baseline_scores\n",
    "\n",
    "# Compute the average of Brier score for each prefix\n",
    "for prefix, scores in brier_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_brier_score.append((prefix, sum_score))\n",
    "\n",
    "for prefix, scores in baseline_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_baseline_score.append((prefix, sum_score))\n",
    "\n",
    "normalized_score_list = []\n",
    "for i in range(len(avg_brier_score)):\n",
    "    normalized_score = avg_brier_score[i][1]/avg_baseline_score[i][1]\n",
    "    cell = (avg_brier_score[i][0], normalized_score)\n",
    "    normalized_score_list.append(cell)\n",
    "\n",
    "\n",
    "print(\"Normalized Brier scores for each categorical variable:\")\n",
    "for prefix, avg_score in normalized_score_list:\n",
    "    print(f\"{prefix}: {avg_score:.2f} \")\n",
    "    cat_normalized_brier.append(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for each outcome (chain - true values):\n",
      "KFSS_M-2y: 0.18 (± 0.02)\n",
      "KFSS_P-2y: 0.25 (± 0.03)\n",
      "EDSS-2y: 0.12 (± 0.01)\n",
      "T25FW-2y: 0.26 (± 0.08)\n",
      "NHPT-2y: 0.45 (± 0.10)\n",
      "P_R36-SF12-after: 0.30 (± 0.02)\n",
      "M_R36-SF12-after: 0.44 (± 0.04)\n",
      "SES_after: 0.27 (± 0.06)\n",
      "SLEC_after: 0.32 (± 0.03)\n",
      "KFSS_M-after_2y: 0.30 (± 0.03)\n",
      "KFSS_P-after_2y: 0.45 (± 0.04)\n",
      "EDSS-after_2y: 0.25 (± 0.01)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows in y_test and y_pred where the variable in question is missing in y_test (since without it, it is not possible to calculate the score)\n",
    "y_test_cv = []\n",
    "y_pred_cv = []\n",
    "\n",
    "for j in range(len(y_test_list)): \n",
    "    y_test_targ = []\n",
    "    y_pred_targ = []\n",
    "    nvar=y_test_list[0].shape[1]\n",
    "\n",
    "    for i in range(0, nvar):  \n",
    "        missing_rows_mask = y_test_list[j].iloc[:, i].isna()\n",
    "        y_test = y_test_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        y_pred = y_pred_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        \n",
    "        y_test_targ.append(y_test)\n",
    "        y_pred_targ.append(y_pred)\n",
    "    \n",
    "    y_test_cv.append(y_test_targ)\n",
    "    y_pred_cv.append(y_pred_targ)\n",
    "\n",
    "\n",
    "\n",
    "# OBTAIN NORMALIZED MSE \n",
    "\n",
    "scores_with_std = []\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for variable_name in variables: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Check if the target variable is numerical or categorical\n",
    "    if y_test_cv[0][variables.index(variable_name)].dtype.kind in 'bifc':\n",
    "        # Scores for the variable across all folds\n",
    "        for fold_index in range(len(y_test_cv)):\n",
    "            y_test = y_test_cv[fold_index][variables.index(variable_name)] \n",
    "            y_pred = y_pred_cv[fold_index][variables.index(variable_name)] \n",
    "            y_train = y_train_list[fold_index][variable_name]\n",
    "\n",
    "            score = normalized_mean_squared_error(y_test, y_pred, y_train)\n",
    "            variable_scores.append(score)\n",
    "        \n",
    "        # Average score for the variable across all folds\n",
    "        variable_avg_score = np.mean(variable_scores)\n",
    "        \n",
    "        # Standard deviation for the variable across all folds\n",
    "        variable_std_score = np.std(variable_scores)\n",
    "        \n",
    "        # Append the tuple with three elements to the scores_with_std list\n",
    "        scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "num_normalized_brier=[]\n",
    "num_std_brier=[]\n",
    "\n",
    "print(\"Scores for each outcome (chain - true values):\")\n",
    "for variable_name, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    num_normalized_brier.append(avg_score)\n",
    "    num_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18041131 0.24833518 0.11963726 0.25913835 0.4547034  0.30254178\n",
      " 0.43511651 0.27334839 0.31975615 0.30364361 0.44583573 0.25138093\n",
      " 1.22908158 1.31905968]\n",
      "Average relative Brier score: 0.45781981528514637\n"
     ]
    }
   ],
   "source": [
    "# Concatenate normalized brier scores for all variables (both numerical and categorical) \n",
    "combined_normalized_brier = np.concatenate((num_normalized_brier, cat_normalized_brier))\n",
    "print(combined_normalized_brier)\n",
    "\n",
    "# Average relative Brier score\n",
    "average_normalized_brier = np.mean(combined_normalized_brier)\n",
    "print(\"Average relative Brier score:\", average_normalized_brier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
