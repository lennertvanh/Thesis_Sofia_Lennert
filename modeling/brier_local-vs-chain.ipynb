{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Relative Brier Score - Local Models vs Model Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compares the Average Relative Brier scores of the local models and the two model chains (where we either propagate the true values or the predictions at fit time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error as mse, brier_score_loss\n",
    "from chaining import Chain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingness_stratified_cv(df, N_FOLDS=5, random_state=None):\n",
    "    # Add seed for reproducibility of the predictions (to get the same scores each time we run the code)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Initial complete-case test fold assignment\n",
    "    cv = pd.Series(np.nan, index=df.index)\n",
    "    i_cc = (df.isna().sum(axis=1) == 0) # Complete cases\n",
    "    cv.iloc[i_cc] = np.random.randint(low=0, high=N_FOLDS, size=i_cc.sum())\n",
    "\n",
    "    # Go over columns from most missing to least missing\n",
    "    for j in df.isna().sum().argsort()[::-1]:\n",
    "        # Instances i that are not assigned yet but for which df[i,j] is observed\n",
    "        i_tbf = (cv.isna()) & (~df.iloc[:,j].isna()) # to be filled\n",
    "        # Fill them randomly\n",
    "        cv.iloc[i_tbf] = np.random.randint(low=0, high=N_FOLDS, size=i_tbf.sum())\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingness_and_categorical_stratified_cv(df, N_FOLDS=5, random_state=None):\n",
    "    # Add seed for reproducibility of the predictions (to get the same scores each time we run the code)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Initial complete-case test fold assignment\n",
    "    cv = pd.Series(np.nan, index=df.index)\n",
    "    i_cc = (df.isna().sum(axis=1) == 0) # Complete cases\n",
    "    cv.iloc[i_cc] = np.random.randint(low=0, high=N_FOLDS, size=i_cc.sum())\n",
    "\n",
    "    # Stratify categorical variables\n",
    "    for col in df.select_dtypes(include=['category']):\n",
    "        counts = df[col].value_counts(normalize=True)\n",
    "        for category in counts.index:\n",
    "            idx = df[col] == category\n",
    "            cv[idx] = cv[idx].fillna(np.random.choice(np.where(idx)[0], size=int(counts[category] * N_FOLDS), replace=False))\n",
    "\n",
    "    # Go over columns from most missing to least missing\n",
    "    for j in df.isna().sum().argsort()[::-1]:\n",
    "        # Instances i that are not assigned yet but for which df[i,j] is observed\n",
    "        i_tbf = (cv.isna()) & (~df.iloc[:,j].isna()) # to be filled\n",
    "        # Fill them randomly\n",
    "        cv.iloc[i_tbf] = np.random.randint(low=0, high=N_FOLDS, size=i_tbf.sum())\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_paths = [\n",
    "    'C:/Users/lenne/OneDrive/Documenten/Master of Statistics and Data Science/2023-2024/Master thesis/Thesis_Sofia_Lennert/new_data',\n",
    "    'C:/Users/anaso/Desktop/SOFIA MENDES/KU Leuven/Master Thesis/Thesis_Sofia_Lennert/new_data'\n",
    "]\n",
    "\n",
    "file = 'merged_data.csv'\n",
    "\n",
    "# Find full paths to the CSV files\n",
    "path = next((f'{path}/{file}' for path in possible_paths if os.path.exists(f'{path}/{file}')), None)\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Bin the number of relapses into 0, 1, 2, 3 and 4+ \n",
    "def bin_column(value):\n",
    "    if value in [0, 1, 2, 3]:\n",
    "        return str(value)\n",
    "    else:\n",
    "        return '4+'\n",
    "data['NRELAP'] = data['NRELAP'].apply(bin_column)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice of target variables, and listed already in the chain order \n",
    "variables = ['KFSS_M-2y', 'KFSS_P-2y', 'EDSS-2y', 'T25FW-2y', 'NHPT-2y', 'P_R36-SF12-after', 'M_R36-SF12-after', \n",
    "             'SES_after', 'SLEC_after', 'KFSS_M-after_2y', 'KFSS_P-after_2y', 'EDSS-after_2y', 'NRELAP', 'CESEV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: once we obtain the best ordering, change the order here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract targets\n",
    "targets = data[variables]\n",
    "\n",
    "# Names of input variables\n",
    "columns_to_keep = ['AGE', 'SEX', 'RACE', 'CONTINENT', 'MHDIAGN', 'CARDIO', 'URINARY', 'MUSCKELET', 'FATIGUE', \n",
    "                    'NHPT-before', 'PASAT_2s-before', 'PASAT_3s-before', 'SDMT-before', 'T25FW-before', 'SLEC_before','SES_before',\n",
    "                    'BDI-before', 'EDSS-before', 'KFSS_M-before', 'KFSS_P-before', 'M_R36-SF12-before',\n",
    "                \t'P_R36-SF12-before', 'R36-SF12-before_Ind', 'T-before','P-before','N-before']\n",
    "\n",
    "# Extract features\n",
    "features = data[columns_to_keep]\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one-hot encoding for categorical and binary input variables\n",
    "object_columns = features.select_dtypes(include=['object'])\n",
    "features = pd.get_dummies(features, columns=object_columns.columns, dtype=int)\n",
    "#features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFSS_M-2y           float64\n",
       "KFSS_P-2y           float64\n",
       "EDSS-2y             float64\n",
       "T25FW-2y            float64\n",
       "NHPT-2y             float64\n",
       "P_R36-SF12-after    float64\n",
       "M_R36-SF12-after    float64\n",
       "SES_after           float64\n",
       "SLEC_after          float64\n",
       "KFSS_M-after_2y     float64\n",
       "KFSS_P-after_2y     float64\n",
       "EDSS-after_2y       float64\n",
       "NRELAP               object\n",
       "CESEV                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random state for reproducibility\n",
    "random_state = 42\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV Fold\n",
       "4.0    510\n",
       "3.0    502\n",
       "0.0    500\n",
       "1.0    495\n",
       "2.0    458\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate CV folds\n",
    "cv=missingness_and_categorical_stratified_cv(targets, N_FOLDS, random_state)\n",
    "cv = cv.to_frame(name=\"CV Fold\")\n",
    "\n",
    "features_cv = pd.merge(features, pd.DataFrame(cv), left_index=True, right_index=True)\n",
    "targets_cv = pd.merge(targets, pd.DataFrame(cv), left_index=True, right_index=True)\n",
    "\n",
    "features_cv['CV Fold'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_mean_squared_error(true, pred, train):\n",
    "    num = mse(true, pred)\n",
    "    mean_value = np.mean(train)\n",
    "    mean = np.full_like(true, mean_value)\n",
    "    den = mse(true, mean)\n",
    "    nmse_loss = num/den\n",
    "\n",
    "    return nmse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with evaluating on CV Fold 1\n",
      "Done with evaluating on CV Fold 2\n",
      "Done with evaluating on CV Fold 3\n",
      "Done with evaluating on CV Fold 4\n",
      "Done with evaluating on CV Fold 5\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "y_train_list = []\n",
    "y_pred_prob_list = []\n",
    "yi_test_dummies_list = []\n",
    "yi_train_dummies_list = []\n",
    "\n",
    "for i in range(0, N_FOLDS): \n",
    "    Xi_train = features_cv[features_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    Xi_test = features_cv[features_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_train = targets_cv[targets_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_test = targets_cv[targets_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    y_test_list.append(pd.DataFrame(yi_test, columns=yi_test.columns, index=yi_test.index))\n",
    "    y_train_list.append(pd.DataFrame(yi_train, columns=yi_train.columns, index=yi_train.index))\n",
    "\n",
    "    # One hot encoding categorical targets of test and train set to be able to compute brier score\n",
    "    subset_yi_test = yi_test.select_dtypes(include=['object'])\n",
    "    yi_test_dummies = pd.get_dummies(subset_yi_test, columns=subset_yi_test.columns, dtype=int)\n",
    "    subset_yi_train = yi_train.select_dtypes(include=['object'])\n",
    "    yi_train_dummies = pd.get_dummies(subset_yi_train, columns=subset_yi_train.columns, dtype=int)\n",
    "    \n",
    "\n",
    "    chain = Chain(\n",
    "        model_reg=RandomForestRegressor(random_state=random_state),\n",
    "        model_clf=RandomForestClassifier(random_state=random_state),\n",
    "        propagate=False, #RUN LOCAL MODELS \n",
    "    )\n",
    "\n",
    "\n",
    "    chain.fit(Xi_train, yi_train, target_types=None) #[\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"clf\",\"clf\"]\n",
    "    y_pred = chain.predict(Xi_test)\n",
    "    y_pred_prob = chain.predict_proba(Xi_test)\n",
    "    y_pred_list.append(y_pred)\n",
    "    y_pred_prob_list.append(y_pred_prob)\n",
    "    yi_test_dummies_list.append(yi_test_dummies)\n",
    "    yi_train_dummies_list.append(yi_train_dummies)\n",
    "    \n",
    "    print(\"Done with evaluating on CV Fold {}\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain values of baseline model\n",
    "yi_train_dummies_avg = []\n",
    "i=0\n",
    " \n",
    "for yi_train_dummies_fold in yi_train_dummies_list:\n",
    "\n",
    "    percentages = yi_train_dummies_fold.sum() / len(yi_train_dummies_fold)\n",
    "\n",
    "    yi_train_dummies_avg_fold = pd.DataFrame(0, index=yi_test_dummies_list[i].index, columns=yi_train_dummies_fold.columns)\n",
    "\n",
    "    # Replace values in each column with the corresponding percentage\n",
    "    for col in yi_train_dummies_avg_fold.columns:\n",
    "        yi_train_dummies_avg_fold[col] = yi_train_dummies_avg_fold[col].apply(lambda x: percentages[col])\n",
    "    \n",
    "    i += 1\n",
    "    yi_train_dummies_avg.append(yi_train_dummies_avg_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-arrange output of predited probabilities\n",
    "concatenated_dfs = []\n",
    "\n",
    "for j, fold in enumerate(y_pred_prob_list):\n",
    "    dfs = []\n",
    "    len_array = 0\n",
    "    \n",
    "    for i, array in enumerate(fold):\n",
    " \n",
    "        col = yi_test_dummies_list[j].columns[len_array:len_array+len(array[0])]\n",
    "        df = pd.DataFrame(array, columns=col, index=yi_test_dummies_list[j].index)\n",
    "        dfs.append(df)\n",
    "        len_array += len(array[0])\n",
    "    \n",
    "    concatenated_df = pd.concat(dfs, axis=1)\n",
    "    concatenated_dfs.append(concatenated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Brier scores for each categorical variable:\n",
      "NRELAP: 0.96 \n",
      "CESEV: 1.69 \n"
     ]
    }
   ],
   "source": [
    "scores_with_std = []\n",
    "avg_brier_score = []\n",
    "avg_baseline_score = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "cat_normalized_brier=[]\n",
    "\n",
    "# Create a dictionary to store the scores for variables with the same letters before the '_'\n",
    "brier_scores_dict = {}\n",
    "baseline_scores_dict = {}\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for level_name in variables_cat: \n",
    "    brier_scores = []\n",
    "    baseline_scores = []\n",
    "    \n",
    "    # Compute scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][level_name] \n",
    "        y_prob = concatenated_dfs[fold_index][level_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][level_name] \n",
    "        \n",
    "        # Compute the Brier score \n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        N_brier_score = brier_score#*N\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "        N_brier_baseline = brier_baseline#*N\n",
    "\n",
    "        # Append the Brier score to the variable scores list\n",
    "        brier_scores.append(N_brier_score)\n",
    "        baseline_scores.append(N_brier_baseline)\n",
    "    \n",
    "    # Check if the variable name has letters before the '_'\n",
    "    prefix = level_name.split('_')[0]\n",
    "    \n",
    "    # Add the normalized Brier scores to the dictionary based on the prefix\n",
    "    if prefix in brier_scores_dict:\n",
    "        brier_scores_dict[prefix].extend(brier_scores)\n",
    "    else:\n",
    "        brier_scores_dict[prefix] = brier_scores\n",
    "\n",
    "    if prefix in baseline_scores_dict:\n",
    "        baseline_scores_dict[prefix].extend(baseline_scores)\n",
    "    else:\n",
    "        baseline_scores_dict[prefix] = baseline_scores\n",
    "\n",
    "\n",
    "# Compute the average of Brier score for each prefix\n",
    "for prefix, scores in brier_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_brier_score.append((prefix, sum_score))\n",
    "\n",
    "for prefix, scores in baseline_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_baseline_score.append((prefix, sum_score))\n",
    "\n",
    "\n",
    "# Compute the relative Brier score\n",
    "normalized_score_list = []\n",
    "for i in range(len(avg_brier_score)):\n",
    "    normalized_score = avg_brier_score[i][1]/avg_baseline_score[i][1]\n",
    "    cell = (avg_brier_score[i][0], normalized_score)\n",
    "    normalized_score_list.append(cell)\n",
    "\n",
    "print(\"Relative Brier scores for each categorical variable:\")\n",
    "for prefix, avg_score in normalized_score_list:\n",
    "    print(f\"{prefix}: {avg_score:.2f} \")\n",
    "    cat_normalized_brier.append(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cv = []\n",
    "y_pred_cv = []\n",
    "\n",
    "for j in range(len(y_test_list)): \n",
    "    y_test_targ = []\n",
    "    y_pred_targ = []\n",
    "    nvar=y_test_list[0].shape[1]\n",
    "\n",
    "    for i in range(0, nvar): \n",
    "        missing_rows_mask = y_test_list[j].iloc[:, i].isna()\n",
    "        y_test = y_test_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        y_pred = y_pred_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        \n",
    "        y_test_targ.append(y_test)\n",
    "        y_pred_targ.append(y_pred)\n",
    "    \n",
    "    y_test_cv.append(y_test_targ)\n",
    "    y_pred_cv.append(y_pred_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for each outcome (local):\n",
      "KFSS_M-2y: 0.19 (± 0.02)\n",
      "KFSS_P-2y: 0.25 (± 0.03)\n",
      "EDSS-2y: 0.12 (± 0.01)\n",
      "T25FW-2y: 0.27 (± 0.08)\n",
      "NHPT-2y: 0.51 (± 0.10)\n",
      "P_R36-SF12-after: 0.31 (± 0.02)\n",
      "M_R36-SF12-after: 0.43 (± 0.03)\n",
      "SES_after: 0.32 (± 0.05)\n",
      "SLEC_after: 0.34 (± 0.04)\n",
      "KFSS_M-after_2y: 0.34 (± 0.04)\n",
      "KFSS_P-after_2y: 0.48 (± 0.06)\n",
      "EDSS-after_2y: 0.24 (± 0.02)\n"
     ]
    }
   ],
   "source": [
    "# Obtain normalized MSE\n",
    "scores_with_std = []\n",
    "\n",
    "for variable_name in variables: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Check if the target variable is numerical or categorical\n",
    "    if y_test_cv[0][variables.index(variable_name)].dtype.kind in 'bifc':\n",
    "        # Compute scores for the variable across all folds\n",
    "        for fold_index in range(len(y_test_cv)):\n",
    "            y_test = y_test_cv[fold_index][variables.index(variable_name)] \n",
    "            y_pred = y_pred_cv[fold_index][variables.index(variable_name)]\n",
    "            y_train = y_train_list[fold_index][variable_name]  \n",
    "            \n",
    "            score = normalized_mean_squared_error(y_test, y_pred, y_train)\n",
    "            variable_scores.append(score)\n",
    "        \n",
    "        # Compute the average score for the variable across all folds\n",
    "        variable_avg_score = np.mean(variable_scores)\n",
    "        \n",
    "        # Compute the standard deviation for the variable across all folds\n",
    "        variable_std_score = np.std(variable_scores)\n",
    "        \n",
    "        scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "num_normalized_brier=[]\n",
    "num_std_brier=[]\n",
    "\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "print(\"Scores for each outcome (local):\")\n",
    "for variable_name, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    num_normalized_brier.append(avg_score)\n",
    "    num_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19211941, 0.24903906, 0.11731794, 0.27467286, 0.5091254 ,\n",
       "       0.30631702, 0.43172177, 0.31852748, 0.33560524, 0.33947974,\n",
       "       0.47959171, 0.24045549, 0.96348948, 1.69499749])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate relative brier scores for all variables (both numerical and categorical) \n",
    "combined_normalized_brier = np.concatenate((num_normalized_brier, cat_normalized_brier))\n",
    "combined_normalized_brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged Relative Brier score: 0.4608900062952653\n"
     ]
    }
   ],
   "source": [
    "# Compute the average\n",
    "average_normalized_brier = np.mean(combined_normalized_brier)\n",
    "print(\"Averaged Relative Brier score:\", average_normalized_brier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with evaluating on CV Fold 1\n",
      "Done with evaluating on CV Fold 2\n",
      "Done with evaluating on CV Fold 3\n",
      "Done with evaluating on CV Fold 4\n",
      "Done with evaluating on CV Fold 5\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "y_train_list = []\n",
    "y_pred_prob_list = []\n",
    "yi_test_dummies_list = []\n",
    "yi_train_dummies_list = []\n",
    "\n",
    "for i in range(0, N_FOLDS): \n",
    "    Xi_train = features_cv[features_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    Xi_test = features_cv[features_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_train = targets_cv[targets_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_test = targets_cv[targets_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    y_test_list.append(pd.DataFrame(yi_test, columns=yi_test.columns, index=yi_test.index))\n",
    "    y_train_list.append(pd.DataFrame(yi_train, columns=yi_train.columns, index=yi_train.index))\n",
    "\n",
    "    # One hot encode categorical targets of test set ans train set to be able to compute brier score\n",
    "    subset_yi_test = yi_test.select_dtypes(include=['object'])\n",
    "    yi_test_dummies = pd.get_dummies(subset_yi_test, columns=subset_yi_test.columns, dtype=int)\n",
    "    subset_yi_train = yi_train.select_dtypes(include=['object'])\n",
    "    yi_train_dummies = pd.get_dummies(subset_yi_train, columns=subset_yi_train.columns, dtype=int)\n",
    "    \n",
    "\n",
    "    chain = Chain(\n",
    "        model_reg=RandomForestRegressor(random_state=random_state),\n",
    "        model_clf=RandomForestClassifier(random_state=random_state),\n",
    "        propagate=\"pred\", #RUN MODELS IN A CHAIN\n",
    "    )\n",
    "\n",
    "\n",
    "    chain.fit(Xi_train, yi_train, target_types=None) #[\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"clf\",\"clf\"]\n",
    "    y_pred = chain.predict(Xi_test)\n",
    "    y_pred_prob = chain.predict_proba(Xi_test)\n",
    "    y_pred_list.append(y_pred)\n",
    "    y_pred_prob_list.append(y_pred_prob)\n",
    "    yi_test_dummies_list.append(yi_test_dummies)\n",
    "    yi_train_dummies_list.append(yi_train_dummies)\n",
    "    \n",
    "    print(\"Done with evaluating on CV Fold {}\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain values of baseline model\n",
    "yi_train_dummies_avg = []\n",
    "i=0\n",
    " \n",
    "for yi_train_dummies_fold in yi_train_dummies_list:\n",
    "\n",
    "    percentages = yi_train_dummies_fold.sum() / len(yi_train_dummies_fold)\n",
    "\n",
    "    yi_train_dummies_avg_fold = pd.DataFrame(0, index=yi_test_dummies_list[i].index, columns=yi_train_dummies_fold.columns)\n",
    "\n",
    "    # Replace values in each column with the corresponding percentage\n",
    "    for col in yi_train_dummies_avg_fold.columns:\n",
    "        yi_train_dummies_avg_fold[col] = yi_train_dummies_avg_fold[col].apply(lambda x: percentages[col])\n",
    "    \n",
    "    i += 1\n",
    "    yi_train_dummies_avg.append(yi_train_dummies_avg_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-arrange output of predicted probabilities\n",
    "concatenated_dfs = []\n",
    "\n",
    "for j, fold in enumerate(y_pred_prob_list):\n",
    "    dfs = []\n",
    "    len_array = 0\n",
    "    \n",
    "    for i, array in enumerate(fold):\n",
    "\n",
    "        col = yi_test_dummies_list[j].columns[len_array:len_array+len(array[0])]\n",
    "        df = pd.DataFrame(array, columns=col, index=yi_test_dummies_list[j].index)\n",
    "        dfs.append(df)\n",
    "        len_array += len(array[0])\n",
    "    \n",
    "\n",
    "    concatenated_df = pd.concat(dfs, axis=1)\n",
    "    concatenated_dfs.append(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = {}\n",
    "\n",
    "for df in yi_test_dummies_list:\n",
    "\n",
    "    for column in df.columns:\n",
    "        # Sum occurrences of 1s in the column and update column_sums\n",
    "        column_sum = df[column].sum()\n",
    "        column_sums[column] = column_sums.get(column, 0) + column_sum\n",
    "\n",
    "total_counts_df = pd.DataFrame(list(column_sums.items()), columns=['Name', 'Total'])\n",
    "#total_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Brier scores for each categorical variable:\n",
      "NRELAP: 0.96 \n",
      "CESEV: 1.70 \n"
     ]
    }
   ],
   "source": [
    "scores_with_std = []\n",
    "avg_brier_score = []\n",
    "avg_baseline_score = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "cat_normalized_brier=[]\n",
    "\n",
    "# Create a dictionary to store the scores for variables with the same letters before the '_'\n",
    "brier_scores_dict = {}\n",
    "baseline_scores_dict = {}\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for level_name in variables_cat: \n",
    "    brier_scores = []\n",
    "    baseline_scores = []\n",
    "    \n",
    "    # Compute scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][level_name] \n",
    "        y_prob = concatenated_dfs[fold_index][level_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][level_name] \n",
    "        \n",
    "        # Compute the Brier score \n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        N_brier_score = brier_score#*N\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "        N_brier_baseline = brier_baseline#*N\n",
    "\n",
    "        # Append the Brier score to the variable scores list\n",
    "        brier_scores.append(N_brier_score)\n",
    "        baseline_scores.append(N_brier_baseline)\n",
    "    \n",
    "    # Check if the variable name has letters before the '_'\n",
    "    prefix = level_name.split('_')[0]\n",
    "    \n",
    "    # Add the Brier scores to the dictionary based on the prefix\n",
    "    if prefix in brier_scores_dict:\n",
    "        brier_scores_dict[prefix].extend(brier_scores)\n",
    "    else:\n",
    "        brier_scores_dict[prefix] = brier_scores\n",
    "\n",
    "    if prefix in baseline_scores_dict:\n",
    "        baseline_scores_dict[prefix].extend(baseline_scores)\n",
    "    else:\n",
    "        baseline_scores_dict[prefix] = baseline_scores\n",
    "\n",
    "\n",
    "# Compute the average of Brier score for each prefix\n",
    "for prefix, scores in brier_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_brier_score.append((prefix, sum_score))\n",
    "\n",
    "for prefix, scores in baseline_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_baseline_score.append((prefix, sum_score))\n",
    "\n",
    "\n",
    "# Compute the relative Brier score\n",
    "normalized_score_list = []\n",
    "for i in range(len(avg_brier_score)):\n",
    "    normalized_score = avg_brier_score[i][1]/avg_baseline_score[i][1]\n",
    "    cell = (avg_brier_score[i][0], normalized_score)\n",
    "    normalized_score_list.append(cell)\n",
    "\n",
    "\n",
    "print(\"Normalized Brier scores for each categorical variable:\")\n",
    "for prefix, avg_score in normalized_score_list:\n",
    "    print(f\"{prefix}: {avg_score:.2f} \")\n",
    "    cat_normalized_brier.append(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for each outcome (chain - propagate predictions):\n",
      "KFSS_M-2y: 0.19 (± 0.02)\n",
      "KFSS_P-2y: 0.25 (± 0.04)\n",
      "EDSS-2y: 0.12 (± 0.01)\n",
      "T25FW-2y: 0.26 (± 0.07)\n",
      "NHPT-2y: 0.45 (± 0.11)\n",
      "P_R36-SF12-after: 0.30 (± 0.02)\n",
      "M_R36-SF12-after: 0.46 (± 0.04)\n",
      "SES_after: 0.32 (± 0.04)\n",
      "SLEC_after: 0.34 (± 0.05)\n",
      "KFSS_M-after_2y: 0.35 (± 0.05)\n",
      "KFSS_P-after_2y: 0.49 (± 0.07)\n",
      "EDSS-after_2y: 0.25 (± 0.02)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows in y_test and y_pred where the variable in question is missing in y_test (since without it, it is not possible to calculate the score)\n",
    "y_test_cv = []\n",
    "y_pred_cv = []\n",
    "\n",
    "for j in range(len(y_test_list)):  # 5\n",
    "    y_test_targ = []\n",
    "    y_pred_targ = []\n",
    "    nvar=y_test_list[0].shape[1]\n",
    "\n",
    "    for i in range(0, nvar):  # or (1, 5)\n",
    "        missing_rows_mask = y_test_list[j].iloc[:, i].isna()\n",
    "        y_test = y_test_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        y_pred = y_pred_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        \n",
    "        y_test_targ.append(y_test)\n",
    "        y_pred_targ.append(y_pred)\n",
    "    \n",
    "    y_test_cv.append(y_test_targ)\n",
    "    y_pred_cv.append(y_pred_targ)\n",
    "\n",
    "\n",
    "# Compute normalized MSE\n",
    "scores_with_std = []\n",
    "\n",
    "for variable_name in variables: \n",
    "    variable_scores = []\n",
    "    \n",
    "    if y_test_cv[0][variables.index(variable_name)].dtype.kind in 'bifc':\n",
    "\n",
    "        for fold_index in range(len(y_test_cv)):\n",
    "            y_test = y_test_cv[fold_index][variables.index(variable_name)] \n",
    "            y_pred = y_pred_cv[fold_index][variables.index(variable_name)]\n",
    "            y_train = y_train_list[fold_index][variable_name] \n",
    "            \n",
    "            score = normalized_mean_squared_error(y_test, y_pred, y_train)\n",
    "            variable_scores.append(score)\n",
    "        \n",
    "        # Compute the average score for the variable across all folds\n",
    "        variable_avg_score = np.mean(variable_scores)\n",
    "        \n",
    "        # Compute the standard deviation for the variable across all folds\n",
    "        variable_std_score = np.std(variable_scores)\n",
    "        \n",
    "        # Append the tuple with three elements to the scores_with_std list\n",
    "        scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "num_normalized_brier=[]\n",
    "num_std_brier=[]\n",
    "\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "print(\"Scores for each outcome (chain - propagate predictions):\")\n",
    "for variable_name, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    num_normalized_brier.append(avg_score)\n",
    "    num_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19211941, 0.2478561 , 0.12063571, 0.25878695, 0.45344982,\n",
       "       0.30471859, 0.45836199, 0.31635688, 0.33914222, 0.3478705 ,\n",
       "       0.49071616, 0.24923144, 0.96346852, 1.69754072])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate normalized brier scores for all variables (both numerical and categorical) \n",
    "combined_normalized_brier = np.concatenate((num_normalized_brier, cat_normalized_brier))\n",
    "combined_normalized_brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Relative Brier score: 0.46001821388100755\n"
     ]
    }
   ],
   "source": [
    "# Compute the average relative Brier score\n",
    "average_normalized_brier = np.mean(combined_normalized_brier)\n",
    "print(\"Average Relative Brier score:\", average_normalized_brier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagate true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with evaluating on CV Fold 1\n",
      "Done with evaluating on CV Fold 2\n",
      "Done with evaluating on CV Fold 3\n",
      "Done with evaluating on CV Fold 4\n",
      "Done with evaluating on CV Fold 5\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "y_train_list = []\n",
    "y_pred_prob_list = []\n",
    "yi_test_dummies_list = []\n",
    "yi_train_dummies_list = []\n",
    "\n",
    "for i in range(0, N_FOLDS): \n",
    "    Xi_train = features_cv[features_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    Xi_test = features_cv[features_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_train = targets_cv[targets_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_test = targets_cv[targets_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    y_test_list.append(pd.DataFrame(yi_test, columns=yi_test.columns, index=yi_test.index))\n",
    "    y_train_list.append(pd.DataFrame(yi_train, columns=yi_train.columns, index=yi_train.index))\n",
    "\n",
    "    # One hot encode categorical targets of test set to be able to compute brier score\n",
    "    subset_yi_test = yi_test.select_dtypes(include=['object'])\n",
    "    yi_test_dummies = pd.get_dummies(subset_yi_test, columns=subset_yi_test.columns, dtype=int)\n",
    "    subset_yi_train = yi_train.select_dtypes(include=['object'])\n",
    "    yi_train_dummies = pd.get_dummies(subset_yi_train, columns=subset_yi_train.columns, dtype=int)\n",
    "    \n",
    "\n",
    "    chain = Chain(\n",
    "        model_reg=RandomForestRegressor(random_state=random_state),\n",
    "        model_clf=RandomForestClassifier(random_state=random_state),\n",
    "        propagate=\"true\", #RUN MODELS IN A CHAIN\n",
    "    )\n",
    "\n",
    "\n",
    "    chain.fit(Xi_train, yi_train, target_types=None) #[\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"clf\",\"clf\"]\n",
    "    y_pred = chain.predict(Xi_test)\n",
    "    y_pred_prob = chain.predict_proba(Xi_test)\n",
    "    y_pred_list.append(y_pred)\n",
    "    y_pred_prob_list.append(y_pred_prob)\n",
    "    yi_test_dummies_list.append(yi_test_dummies)\n",
    "    yi_train_dummies_list.append(yi_train_dummies)\n",
    "    \n",
    "    print(\"Done with evaluating on CV Fold {}\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain values of baseline model\n",
    "yi_train_dummies_avg = []\n",
    "i=0\n",
    "\n",
    "for yi_train_dummies_fold in yi_train_dummies_list:\n",
    "\n",
    "    percentages = yi_train_dummies_fold.sum() / len(yi_train_dummies_fold)\n",
    "\n",
    "    yi_train_dummies_avg_fold = pd.DataFrame(0, index=yi_test_dummies_list[i].index, columns=yi_train_dummies_fold.columns)\n",
    "\n",
    "    # Replace values in each column with the corresponding percentage\n",
    "    for col in yi_train_dummies_avg_fold.columns:\n",
    "        yi_train_dummies_avg_fold[col] = yi_train_dummies_avg_fold[col].apply(lambda x: percentages[col])\n",
    "    \n",
    "    i += 1\n",
    "    yi_train_dummies_avg.append(yi_train_dummies_avg_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_dfs = []\n",
    "\n",
    "for j, fold in enumerate(y_pred_prob_list):\n",
    "    dfs = []\n",
    "    len_array = 0\n",
    "    \n",
    "    for i, array in enumerate(fold):\n",
    "\n",
    "        col = yi_test_dummies_list[j].columns[len_array:len_array+len(array[0])]\n",
    "        df = pd.DataFrame(array, columns=col, index=yi_test_dummies_list[j].index)\n",
    "        dfs.append(df)\n",
    "        len_array += len(array[0])\n",
    "    \n",
    "    concatenated_df = pd.concat(dfs, axis=1)\n",
    "    concatenated_dfs.append(concatenated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sums = {}\n",
    "\n",
    "for df in yi_test_dummies_list:\n",
    "\n",
    "    for column in df.columns:\n",
    "        # Sum occurrences of 1s in the column and update column_sums\n",
    "        column_sum = df[column].sum()\n",
    "        column_sums[column] = column_sums.get(column, 0) + column_sum\n",
    "\n",
    "total_counts_df = pd.DataFrame(list(column_sums.items()), columns=['Name', 'Total'])\n",
    "#total_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Brier scores for each categorical variable:\n",
      "NRELAP: 0.95 \n",
      "CESEV: 1.65 \n"
     ]
    }
   ],
   "source": [
    "scores_with_std = []\n",
    "avg_brier_score = []\n",
    "avg_baseline_score = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "cat_normalized_brier=[]\n",
    "\n",
    "# Create a dictionary to store the scores for variables with the same letters before the '_'\n",
    "brier_scores_dict = {}\n",
    "baseline_scores_dict = {}\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for level_name in variables_cat: \n",
    "    brier_scores = []\n",
    "    baseline_scores = []\n",
    "    \n",
    "    # Compute scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][level_name] \n",
    "        y_prob = concatenated_dfs[fold_index][level_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][level_name] \n",
    "        \n",
    "        # Compute the Brier score \n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        N_brier_score = brier_score#*N\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "        N_brier_baseline = brier_baseline#*N\n",
    "\n",
    "        # Append the Brier score to the variable scores list\n",
    "        brier_scores.append(N_brier_score)\n",
    "        baseline_scores.append(N_brier_baseline)\n",
    "    \n",
    "    # Check if the variable name has letters before the '_'\n",
    "    prefix = level_name.split('_')[0]\n",
    "    \n",
    "    # Add the Brier scores to the dictionary based on the prefix\n",
    "    if prefix in brier_scores_dict:\n",
    "        brier_scores_dict[prefix].extend(brier_scores)\n",
    "    else:\n",
    "        brier_scores_dict[prefix] = brier_scores\n",
    "\n",
    "    if prefix in baseline_scores_dict:\n",
    "        baseline_scores_dict[prefix].extend(baseline_scores)\n",
    "    else:\n",
    "        baseline_scores_dict[prefix] = baseline_scores\n",
    "\n",
    "\n",
    "# Compute the average of Brier score for each prefix\n",
    "for prefix, scores in brier_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_brier_score.append((prefix, sum_score))\n",
    "\n",
    "for prefix, scores in baseline_scores_dict.items():\n",
    "    sum_score = np.sum(scores)\n",
    "    avg_baseline_score.append((prefix, sum_score))\n",
    "\n",
    "\n",
    "# Compute normalized Brier score\n",
    "normalized_score_list = []\n",
    "for i in range(len(avg_brier_score)):\n",
    "    normalized_score = avg_brier_score[i][1]/avg_baseline_score[i][1]\n",
    "    cell = (avg_brier_score[i][0], normalized_score)\n",
    "    normalized_score_list.append(cell)\n",
    "\n",
    "\n",
    "print(\"Normalized Brier scores for each categorical variable:\")\n",
    "for prefix, avg_score in normalized_score_list:\n",
    "    print(f\"{prefix}: {avg_score:.2f} \")\n",
    "    cat_normalized_brier.append(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows in y_test and y_pred where the variable in question is missing in y_test (since without it, it is not possible to calculate the score)\n",
    "y_test_cv = []\n",
    "y_pred_cv = []\n",
    "\n",
    "for j in range(len(y_test_list)):  # 5\n",
    "    y_test_targ = []\n",
    "    y_pred_targ = []\n",
    "    nvar=y_test_list[0].shape[1]\n",
    "\n",
    "    for i in range(0, nvar):  # or (1, 5)\n",
    "        missing_rows_mask = y_test_list[j].iloc[:, i].isna()\n",
    "        y_test = y_test_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        y_pred = y_pred_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        \n",
    "        y_test_targ.append(y_test)\n",
    "        y_pred_targ.append(y_pred)\n",
    "    \n",
    "    y_test_cv.append(y_test_targ)\n",
    "    y_pred_cv.append(y_pred_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for each outcome (chain - true values):\n",
      "KFSS_M-2y: 0.19 (± 0.02)\n",
      "KFSS_P-2y: 0.25 (± 0.03)\n",
      "EDSS-2y: 0.12 (± 0.01)\n",
      "T25FW-2y: 0.25 (± 0.08)\n",
      "NHPT-2y: 0.45 (± 0.11)\n",
      "P_R36-SF12-after: 0.30 (± 0.02)\n",
      "M_R36-SF12-after: 0.45 (± 0.04)\n",
      "SES_after: 0.31 (± 0.05)\n",
      "SLEC_after: 0.34 (± 0.04)\n",
      "KFSS_M-after_2y: 0.33 (± 0.05)\n",
      "KFSS_P-after_2y: 0.48 (± 0.04)\n",
      "EDSS-after_2y: 0.24 (± 0.02)\n"
     ]
    }
   ],
   "source": [
    "scores_with_std = []\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for variable_name in variables: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Check if the target variable is numerical or categorical\n",
    "    if y_test_cv[0][variables.index(variable_name)].dtype.kind in 'bifc':\n",
    "        # Compute scores for the variable across all folds\n",
    "        for fold_index in range(len(y_test_cv)):\n",
    "            y_test = y_test_cv[fold_index][variables.index(variable_name)] \n",
    "            y_pred = y_pred_cv[fold_index][variables.index(variable_name)] \n",
    "            y_train = y_train_list[fold_index][variable_name]\n",
    "            \n",
    "            score = normalized_mean_squared_error(y_test, y_pred, y_train)\n",
    "            variable_scores.append(score)\n",
    "        \n",
    "        # Compute the average score for the variable across all folds\n",
    "        variable_avg_score = np.mean(variable_scores)\n",
    "        \n",
    "        # Compute the standard deviation for the variable across all folds\n",
    "        variable_std_score = np.std(variable_scores)\n",
    "        \n",
    "        # Append the tuple with three elements to the scores_with_std list\n",
    "        scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "num_normalized_brier=[]\n",
    "num_std_brier=[]\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "print(\"Scores for each outcome (chain - true values):\")\n",
    "for variable_name, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    num_normalized_brier.append(avg_score)\n",
    "    num_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19211941, 0.24852978, 0.11698552, 0.25482681, 0.44970997,\n",
       "       0.29725601, 0.44935074, 0.31452837, 0.33911912, 0.33465669,\n",
       "       0.48131218, 0.23854652, 0.9482    , 1.65100328])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate normalized brier scores for all variables (both numerical and categorical) \n",
    "combined_normalized_brier = np.concatenate((num_normalized_brier, cat_normalized_brier))\n",
    "combined_normalized_brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Relative Brier score: 0.45115317130496885\n"
     ]
    }
   ],
   "source": [
    "# Compute the average relative Brier score\n",
    "average_normalized_brier = np.mean(combined_normalized_brier)\n",
    "print(\"Average Relative Brier score:\", average_normalized_brier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
