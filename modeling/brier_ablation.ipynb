{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compares the scores of the local models and the model chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import itertools\n",
    "#import random\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, accuracy_score, mean_squared_error as mse, brier_score_loss\n",
    "from chaining import Chain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingness_stratified_cv(df, N_FOLDS=5, random_state=None):\n",
    "    # Add seed for reproducibility of the predictions (to get the same scores each time we run the code)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Initial complete-case test fold assignment\n",
    "    cv = pd.Series(np.nan, index=df.index)\n",
    "    i_cc = (df.isna().sum(axis=1) == 0) # Complete cases\n",
    "    cv.iloc[i_cc] = np.random.randint(low=0, high=N_FOLDS, size=i_cc.sum())\n",
    "\n",
    "    # Go over columns from most missing to least missing\n",
    "    for j in df.isna().sum().argsort()[::-1]:\n",
    "        # Instances i that are not assigned yet but for which df[i,j] is observed\n",
    "        i_tbf = (cv.isna()) & (~df.iloc[:,j].isna()) # to be filled\n",
    "        # Fill them randomly\n",
    "        cv.iloc[i_tbf] = np.random.randint(low=0, high=N_FOLDS, size=i_tbf.sum())\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingness_and_categorical_stratified_cv(df, N_FOLDS=5, random_state=None):\n",
    "    # Add seed for reproducibility of the predictions (to get the same scores each time we run the code)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Initial complete-case test fold assignment\n",
    "    cv = pd.Series(np.nan, index=df.index)\n",
    "    i_cc = (df.isna().sum(axis=1) == 0) # Complete cases\n",
    "    cv.iloc[i_cc] = np.random.randint(low=0, high=N_FOLDS, size=i_cc.sum())\n",
    "\n",
    "    # Stratify categorical variables\n",
    "    for col in df.select_dtypes(include=['category']):\n",
    "        counts = df[col].value_counts(normalize=True)\n",
    "        for category in counts.index:\n",
    "            idx = df[col] == category\n",
    "            cv[idx] = cv[idx].fillna(np.random.choice(np.where(idx)[0], size=int(counts[category] * N_FOLDS), replace=False))\n",
    "\n",
    "    # Go over columns from most missing to least missing\n",
    "    for j in df.isna().sum().argsort()[::-1]:\n",
    "        # Instances i that are not assigned yet but for which df[i,j] is observed\n",
    "        i_tbf = (cv.isna()) & (~df.iloc[:,j].isna()) # to be filled\n",
    "        # Fill them randomly\n",
    "        cv.iloc[i_tbf] = np.random.randint(low=0, high=N_FOLDS, size=i_tbf.sum())\n",
    "\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USUBJID</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RACE</th>\n",
       "      <th>CONTINENT</th>\n",
       "      <th>CESEV</th>\n",
       "      <th>CECONTRT</th>\n",
       "      <th>TOTRELAP</th>\n",
       "      <th>MHCONTRT</th>\n",
       "      <th>MHDIAGN</th>\n",
       "      <th>CARDIO</th>\n",
       "      <th>URINARY</th>\n",
       "      <th>MUSCKELET</th>\n",
       "      <th>FATIGUE</th>\n",
       "      <th>SMSTDY</th>\n",
       "      <th>NRELAP</th>\n",
       "      <th>NHPT-before</th>\n",
       "      <th>NHPT-2y</th>\n",
       "      <th>NHPT-after_2y</th>\n",
       "      <th>PASAT_2s-before</th>\n",
       "      <th>PASAT_2s-2y</th>\n",
       "      <th>PASAT_2s-after_2y</th>\n",
       "      <th>PASAT_3s-before</th>\n",
       "      <th>PASAT_3s-2y</th>\n",
       "      <th>PASAT_3s-after_2y</th>\n",
       "      <th>SDMT-before</th>\n",
       "      <th>SDMT-2y</th>\n",
       "      <th>T25FW-before</th>\n",
       "      <th>T25FW-2y</th>\n",
       "      <th>T25FW-after_2y</th>\n",
       "      <th>T-before</th>\n",
       "      <th>T-after</th>\n",
       "      <th>P-before</th>\n",
       "      <th>P-after</th>\n",
       "      <th>N-before</th>\n",
       "      <th>N-after</th>\n",
       "      <th>SLEC_before</th>\n",
       "      <th>SLEC_after</th>\n",
       "      <th>SES_after</th>\n",
       "      <th>SES_before</th>\n",
       "      <th>VAA</th>\n",
       "      <th>BDI-before</th>\n",
       "      <th>BDI-after</th>\n",
       "      <th>EDSS-before</th>\n",
       "      <th>EDSS-2y</th>\n",
       "      <th>EDSS-after_2y</th>\n",
       "      <th>KFSS1-Sensory-2y</th>\n",
       "      <th>KFSS1-Sensory-after_2y</th>\n",
       "      <th>KFSS1-Sensory-before</th>\n",
       "      <th>KFSS1-Brain-2y</th>\n",
       "      <th>KFSS1-Brain-after_2y</th>\n",
       "      <th>KFSS1-Brain-before</th>\n",
       "      <th>KFSS1-Bowel-2y</th>\n",
       "      <th>KFSS1-Bowel-after_2y</th>\n",
       "      <th>KFSS1-Bowel-before</th>\n",
       "      <th>KFSS1-Pyramidal-2y</th>\n",
       "      <th>KFSS1-Pyramidal-after_2y</th>\n",
       "      <th>KFSS1-Pyramidal-before</th>\n",
       "      <th>KFSS1-Cerebral-2y</th>\n",
       "      <th>KFSS1-Cerebral-after_2y</th>\n",
       "      <th>KFSS1-Cerebral-before</th>\n",
       "      <th>KFSS1-Visual-2y</th>\n",
       "      <th>KFSS1-Visual-after_2y</th>\n",
       "      <th>KFSS1-Visual-before</th>\n",
       "      <th>KFSS1-Cerebellar-2y</th>\n",
       "      <th>KFSS1-Cerebellar-after_2y</th>\n",
       "      <th>KFSS1-Cerebellar-before</th>\n",
       "      <th>KFSS_M-2y</th>\n",
       "      <th>KFSS_M-after_2y</th>\n",
       "      <th>KFSS_M-before</th>\n",
       "      <th>KFSS_P-2y</th>\n",
       "      <th>KFSS_P-after_2y</th>\n",
       "      <th>KFSS_P-before</th>\n",
       "      <th>M_R36-SF12-before</th>\n",
       "      <th>P_R36-SF12-before</th>\n",
       "      <th>R36-SF12-before_Ind</th>\n",
       "      <th>M_R36-SF12-after</th>\n",
       "      <th>P_R36-SF12-after</th>\n",
       "      <th>R36-SF12-after_Ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSOAC/0014</td>\n",
       "      <td>46.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSOAC/0016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NORTH AMERICA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>SPMS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.55</td>\n",
       "      <td>6.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSOAC/0019</td>\n",
       "      <td>44.0</td>\n",
       "      <td>M</td>\n",
       "      <td>NON-WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PPMS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.65</td>\n",
       "      <td>21.30</td>\n",
       "      <td>20.15</td>\n",
       "      <td>34.5</td>\n",
       "      <td>35.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>43.5</td>\n",
       "      <td>51.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.30</td>\n",
       "      <td>6.15</td>\n",
       "      <td>5.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.772152</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSOAC/0024</td>\n",
       "      <td>60.0</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NORTH AMERICA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SPMS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.45</td>\n",
       "      <td>37.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.569620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MSOAC/0030</td>\n",
       "      <td>28.0</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>EUROPE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.55</td>\n",
       "      <td>17.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.5</td>\n",
       "      <td>69.0</td>\n",
       "      <td>4.85</td>\n",
       "      <td>4.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>0.039683</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>MSOAC/9986</td>\n",
       "      <td>46.0</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>OCEANIA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.35</td>\n",
       "      <td>18.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.90</td>\n",
       "      <td>3.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>MSOAC/9987</td>\n",
       "      <td>18.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>MSOAC/9995</td>\n",
       "      <td>38.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MILD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>MSOAC/9998</td>\n",
       "      <td>40.0</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Y</td>\n",
       "      <td>PPMS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.80</td>\n",
       "      <td>22.40</td>\n",
       "      <td>22.50</td>\n",
       "      <td>21.5</td>\n",
       "      <td>30.5</td>\n",
       "      <td>33.5</td>\n",
       "      <td>31.5</td>\n",
       "      <td>39.5</td>\n",
       "      <td>40.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>MSOAC/9999</td>\n",
       "      <td>38.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MILD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2465 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         USUBJID   AGE SEX       RACE      CONTINENT CESEV CECONTRT  TOTRELAP  \\\n",
       "0     MSOAC/0014  46.0   F        NaN            NaN   NaN      NaN       NaN   \n",
       "1     MSOAC/0016   NaN   M      WHITE  NORTH AMERICA   NaN      NaN       NaN   \n",
       "2     MSOAC/0019  44.0   M  NON-WHITE            NaN   NaN      NaN       NaN   \n",
       "3     MSOAC/0024  60.0   M      WHITE  NORTH AMERICA   NaN      NaN       NaN   \n",
       "4     MSOAC/0030  28.0   F      WHITE         EUROPE   NaN      NaN       NaN   \n",
       "...          ...   ...  ..        ...            ...   ...      ...       ...   \n",
       "2460  MSOAC/9986  46.0   M      WHITE        OCEANIA   NaN      NaN       NaN   \n",
       "2461  MSOAC/9987  18.0   F        NaN            NaN   NaN      NaN       NaN   \n",
       "2462  MSOAC/9995  38.0   F        NaN            NaN  MILD      NaN       4.0   \n",
       "2463  MSOAC/9998  40.0   F      WHITE            NaN   NaN        Y       2.0   \n",
       "2464  MSOAC/9999  38.0   F        NaN            NaN  MILD      NaN       2.0   \n",
       "\n",
       "     MHCONTRT MHDIAGN  CARDIO  URINARY  MUSCKELET  FATIGUE  SMSTDY NRELAP  \\\n",
       "0         NaN    RRMS       0        0          0        0     NaN    0.0   \n",
       "1           Y    SPMS       1        1          0        1     NaN    0.0   \n",
       "2         NaN    PPMS       1        1          0        0     NaN    0.0   \n",
       "3         NaN    SPMS       1        1          1        1     NaN    0.0   \n",
       "4         NaN    RRMS       1        1          0        1     NaN    0.0   \n",
       "...       ...     ...     ...      ...        ...      ...     ...    ...   \n",
       "2460      NaN    RRMS       1        1          0        1     NaN    0.0   \n",
       "2461      NaN    RRMS       0        0          0        0     NaN    0.0   \n",
       "2462      NaN    RRMS       0        0          0        0   142.0    2.0   \n",
       "2463        Y    PPMS       0        1          0        1    79.0    1.0   \n",
       "2464      NaN    RRMS       0        0          0        0    69.0    2.0   \n",
       "\n",
       "      NHPT-before  NHPT-2y  NHPT-after_2y  PASAT_2s-before  PASAT_2s-2y  \\\n",
       "0             NaN      NaN            NaN              NaN          NaN   \n",
       "1             NaN      NaN            NaN              NaN          NaN   \n",
       "2           23.65    21.30          20.15             34.5         35.5   \n",
       "3           34.45    37.50            NaN             55.0         54.0   \n",
       "4           16.55    17.90            NaN              NaN          NaN   \n",
       "...           ...      ...            ...              ...          ...   \n",
       "2460        19.35    18.95            NaN              NaN          NaN   \n",
       "2461          NaN      NaN            NaN              NaN          NaN   \n",
       "2462          NaN      NaN            NaN              NaN          NaN   \n",
       "2463        23.80    22.40          22.50             21.5         30.5   \n",
       "2464          NaN      NaN            NaN              NaN          NaN   \n",
       "\n",
       "      PASAT_2s-after_2y  PASAT_3s-before  PASAT_3s-2y  PASAT_3s-after_2y  \\\n",
       "0                   NaN              NaN          NaN                NaN   \n",
       "1                   NaN              NaN          NaN                NaN   \n",
       "2                  43.0             43.5         51.0               53.0   \n",
       "3                   NaN             60.0         60.0                NaN   \n",
       "4                   NaN             58.0         60.0                NaN   \n",
       "...                 ...              ...          ...                ...   \n",
       "2460                NaN             58.0         60.0                NaN   \n",
       "2461                NaN              NaN          NaN                NaN   \n",
       "2462                NaN              NaN          NaN                NaN   \n",
       "2463               33.5             31.5         39.5               40.5   \n",
       "2464                NaN              NaN          NaN                NaN   \n",
       "\n",
       "      SDMT-before  SDMT-2y  T25FW-before  T25FW-2y  T25FW-after_2y  T-before  \\\n",
       "0             NaN      NaN           NaN       NaN             NaN       NaN   \n",
       "1             NaN      NaN          8.55      6.60             NaN       0.0   \n",
       "2             NaN      NaN          6.30      6.15            5.85       0.0   \n",
       "3             NaN      NaN          4.50      5.25             NaN       0.0   \n",
       "4            63.5     69.0          4.85      4.70             NaN       0.0   \n",
       "...           ...      ...           ...       ...             ...       ...   \n",
       "2460         51.0     60.0          3.90      3.80             NaN       0.0   \n",
       "2461          NaN      NaN           NaN       NaN             NaN       NaN   \n",
       "2462          NaN      NaN           NaN       NaN             NaN       NaN   \n",
       "2463          NaN      NaN          6.15      6.00            6.20       0.0   \n",
       "2464          NaN      NaN           NaN       NaN             NaN       NaN   \n",
       "\n",
       "      T-after  P-before  P-after  N-before  N-after  SLEC_before  SLEC_after  \\\n",
       "0         NaN       NaN      NaN       NaN      NaN          NaN         NaN   \n",
       "1         0.0       NaN      NaN       NaN      NaN          NaN         NaN   \n",
       "2         0.0       0.0      0.0       NaN      NaN          NaN         NaN   \n",
       "3         0.0       0.0      0.0       1.0      0.0          NaN         NaN   \n",
       "4         0.0       0.0      0.0       0.0      0.0         26.0        24.0   \n",
       "...       ...       ...      ...       ...      ...          ...         ...   \n",
       "2460      0.0       0.0      0.0       0.0      0.0         36.0        35.0   \n",
       "2461      NaN       NaN      NaN       NaN      NaN          NaN         NaN   \n",
       "2462      NaN       NaN      NaN       NaN      NaN          NaN         NaN   \n",
       "2463      0.0       0.0      0.0       NaN      NaN          NaN         NaN   \n",
       "2464      NaN       NaN      NaN       NaN      NaN          NaN         NaN   \n",
       "\n",
       "      SES_after  SES_before  VAA  BDI-before  BDI-after  EDSS-before  EDSS-2y  \\\n",
       "0           NaN         NaN  0.0         NaN        NaN          NaN      NaN   \n",
       "1           NaN         NaN  NaN         NaN        NaN         6.00      NaN   \n",
       "2           NaN         NaN  NaN         NaN        NaN         3.75     3.50   \n",
       "3           NaN         NaN  NaN    0.031746   0.023810         4.00     3.75   \n",
       "4          1.25        1.25  NaN    0.063492   0.039683         2.00     1.50   \n",
       "...         ...         ...  ...         ...        ...          ...      ...   \n",
       "2460       1.25        1.25  NaN    0.047619   0.063492         2.75     2.50   \n",
       "2461        NaN         NaN  0.0         NaN        NaN          NaN      NaN   \n",
       "2462        NaN         NaN  0.0         NaN        NaN          NaN      NaN   \n",
       "2463        NaN         NaN  NaN         NaN        NaN         4.50     3.75   \n",
       "2464        NaN         NaN  0.0         NaN        NaN          NaN      NaN   \n",
       "\n",
       "      EDSS-after_2y  KFSS1-Sensory-2y  KFSS1-Sensory-after_2y  \\\n",
       "0               NaN               NaN                     NaN   \n",
       "1               NaN               NaN                     NaN   \n",
       "2               3.0          0.333333                0.166667   \n",
       "3               NaN          0.333333                     NaN   \n",
       "4               NaN          0.166667                     NaN   \n",
       "...             ...               ...                     ...   \n",
       "2460            NaN          0.333333                     NaN   \n",
       "2461            NaN               NaN                     NaN   \n",
       "2462            NaN               NaN                     NaN   \n",
       "2463            4.0          0.166667                0.250000   \n",
       "2464            NaN               NaN                     NaN   \n",
       "\n",
       "      KFSS1-Sensory-before  KFSS1-Brain-2y  KFSS1-Brain-after_2y  \\\n",
       "0                      NaN             NaN                   NaN   \n",
       "1                      NaN             NaN                   NaN   \n",
       "2                 0.500000             0.2                   0.0   \n",
       "3                 0.333333             0.0                   NaN   \n",
       "4                 0.166667             0.2                   NaN   \n",
       "...                    ...             ...                   ...   \n",
       "2460              0.166667             0.0                   NaN   \n",
       "2461                   NaN             NaN                   NaN   \n",
       "2462                   NaN             NaN                   NaN   \n",
       "2463              0.333333             0.4                   0.6   \n",
       "2464                   NaN             NaN                   NaN   \n",
       "\n",
       "      KFSS1-Brain-before  KFSS1-Bowel-2y  KFSS1-Bowel-after_2y  \\\n",
       "0                    NaN             NaN                   NaN   \n",
       "1                    NaN             NaN                   NaN   \n",
       "2                    0.2        0.000000              0.166667   \n",
       "3                    0.1        0.583333                   NaN   \n",
       "4                    0.2        0.166667                   NaN   \n",
       "...                  ...             ...                   ...   \n",
       "2460                 0.0        0.333333                   NaN   \n",
       "2461                 NaN             NaN                   NaN   \n",
       "2462                 NaN             NaN                   NaN   \n",
       "2463                 0.6        0.166667              0.166667   \n",
       "2464                 NaN             NaN                   NaN   \n",
       "\n",
       "      KFSS1-Bowel-before  KFSS1-Pyramidal-2y  KFSS1-Pyramidal-after_2y  \\\n",
       "0                    NaN                 NaN                       NaN   \n",
       "1                    NaN                 NaN                       NaN   \n",
       "2               0.083333            0.333333                       0.5   \n",
       "3               0.666667            0.166667                       NaN   \n",
       "4               0.166667            0.166667                       NaN   \n",
       "...                  ...                 ...                       ...   \n",
       "2460            0.250000            0.166667                       NaN   \n",
       "2461                 NaN                 NaN                       NaN   \n",
       "2462                 NaN                 NaN                       NaN   \n",
       "2463            0.166667            0.333333                       0.5   \n",
       "2464                 NaN                 NaN                       NaN   \n",
       "\n",
       "      KFSS1-Pyramidal-before  KFSS1-Cerebral-2y  KFSS1-Cerebral-after_2y  \\\n",
       "0                        NaN                NaN                      NaN   \n",
       "1                        NaN                NaN                      NaN   \n",
       "2                   0.416667                0.0                      0.0   \n",
       "3                   0.250000                0.0                      NaN   \n",
       "4                   0.333333                0.0                      NaN   \n",
       "...                      ...                ...                      ...   \n",
       "2460                0.333333                0.0                      NaN   \n",
       "2461                     NaN                NaN                      NaN   \n",
       "2462                     NaN                NaN                      NaN   \n",
       "2463                0.500000                0.0                      0.0   \n",
       "2464                     NaN                NaN                      NaN   \n",
       "\n",
       "      KFSS1-Cerebral-before  KFSS1-Visual-2y  KFSS1-Visual-after_2y  \\\n",
       "0                       NaN              NaN                    NaN   \n",
       "1                       NaN              NaN                    NaN   \n",
       "2                       0.0         0.333333                    0.0   \n",
       "3                       0.0         0.000000                    NaN   \n",
       "4                       0.2         0.166667                    NaN   \n",
       "...                     ...              ...                    ...   \n",
       "2460                    0.0         0.000000                    NaN   \n",
       "2461                    NaN              NaN                    NaN   \n",
       "2462                    NaN              NaN                    NaN   \n",
       "2463                    0.4         0.166667                    0.0   \n",
       "2464                    NaN              NaN                    NaN   \n",
       "\n",
       "      KFSS1-Visual-before  KFSS1-Cerebellar-2y  KFSS1-Cerebellar-after_2y  \\\n",
       "0                     NaN                  NaN                        NaN   \n",
       "1                     NaN                  NaN                        NaN   \n",
       "2                0.333333                  0.0                        0.2   \n",
       "3                0.083333                  0.2                        NaN   \n",
       "4                0.083333                  0.0                        NaN   \n",
       "...                   ...                  ...                        ...   \n",
       "2460             0.000000                  0.0                        NaN   \n",
       "2461                  NaN                  NaN                        NaN   \n",
       "2462                  NaN                  NaN                        NaN   \n",
       "2463             0.166667                  0.6                        0.6   \n",
       "2464                  NaN                  NaN                        NaN   \n",
       "\n",
       "      KFSS1-Cerebellar-before  KFSS_M-2y  KFSS_M-after_2y  KFSS_M-before  \\\n",
       "0                         NaN        NaN              NaN            NaN   \n",
       "1                         NaN        NaN              NaN            NaN   \n",
       "2                         0.0   0.185185         0.185185       0.240741   \n",
       "3                         0.5   0.129630              NaN       0.240741   \n",
       "4                         0.1   0.111111              NaN       0.203704   \n",
       "...                       ...        ...              ...            ...   \n",
       "2460                      0.2   0.111111              NaN       0.148148   \n",
       "2461                      NaN        NaN              NaN            NaN   \n",
       "2462                      NaN        NaN              NaN            NaN   \n",
       "2463                      0.6   0.314815         0.351852       0.481481   \n",
       "2464                      NaN        NaN              NaN            NaN   \n",
       "\n",
       "      KFSS_P-2y  KFSS_P-after_2y  KFSS_P-before  M_R36-SF12-before  \\\n",
       "0           NaN              NaN            NaN                NaN   \n",
       "1           NaN              NaN            NaN                NaN   \n",
       "2      0.166667         0.083333       0.208333           0.828571   \n",
       "3      0.291667              NaN       0.375000           0.885714   \n",
       "4      0.166667              NaN       0.125000           0.933333   \n",
       "...         ...              ...            ...                ...   \n",
       "2460   0.166667              NaN       0.125000           0.833333   \n",
       "2461        NaN              NaN            NaN                NaN   \n",
       "2462        NaN              NaN            NaN                NaN   \n",
       "2463   0.166667         0.083333       0.166667           0.728571   \n",
       "2464        NaN              NaN            NaN                NaN   \n",
       "\n",
       "      P_R36-SF12-before  R36-SF12-before_Ind  M_R36-SF12-after  \\\n",
       "0                   NaN                  NaN               NaN   \n",
       "1                   NaN                  NaN               NaN   \n",
       "2              0.772152                  1.0          0.857143   \n",
       "3              0.569620                  1.0          0.857143   \n",
       "4              0.846154                  0.0          0.833333   \n",
       "...                 ...                  ...               ...   \n",
       "2460           0.730769                  0.0          0.800000   \n",
       "2461                NaN                  NaN               NaN   \n",
       "2462                NaN                  NaN               NaN   \n",
       "2463           0.658228                  1.0          0.757143   \n",
       "2464                NaN                  NaN               NaN   \n",
       "\n",
       "      P_R36-SF12-after  R36-SF12-after_Ind  \n",
       "0                  NaN                 NaN  \n",
       "1                  NaN                 NaN  \n",
       "2             0.721519                 1.0  \n",
       "3             0.716216                 1.0  \n",
       "4             0.730769                 0.0  \n",
       "...                ...                 ...  \n",
       "2460          0.750000                 0.0  \n",
       "2461               NaN                 NaN  \n",
       "2462               NaN                 NaN  \n",
       "2463          0.594937                 1.0  \n",
       "2464               NaN                 NaN  \n",
       "\n",
       "[2465 rows x 79 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_paths = [\n",
    "    'C:/Users/lenne/OneDrive/Documenten/Master of Statistics and Data Science/2023-2024/Master thesis/Thesis_Sofia_Lennert/new_data',\n",
    "    'C:/Users/anaso/Desktop/SOFIA MENDES/KU Leuven/Master Thesis/Thesis_Sofia_Lennert/new_data'\n",
    "]\n",
    "\n",
    "# Define file names\n",
    "file = 'merged_data.csv'\n",
    "\n",
    "# Find full paths to the CSV files\n",
    "path = next((f'{path}/{file}' for path in possible_paths if os.path.exists(f'{path}/{file}')), None)\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Bin the number of relapses into 0, 1, 2, 3 and 4+ \n",
    "def bin_column(value):\n",
    "    if value in [0, 1, 2, 3]:\n",
    "        return str(value)\n",
    "    else:\n",
    "        return '4+'\n",
    "data['NRELAP'] = data['NRELAP'].apply(bin_column)\n",
    "\n",
    "# Resulting DataFrame will have aggregated data from all four datasets based on the specific_column\n",
    "pd.set_option('display.max_columns', None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables = ['KFSS_M-2y', 'EDSS-2y', 'T25FW-2y', 'NRELAP']# removed KFSS_P-2y for now -- ('SMSTDY' gave a score of -0.03)\n",
    "variables = ['KFSS_M-2y', 'KFSS_P-2y', 'EDSS-2y', 'T25FW-2y', 'NHPT-2y', 'P_R36-SF12-after', 'M_R36-SF12-after', \n",
    "             'SES_after', 'SLEC_after', 'KFSS_M-after_2y', 'KFSS_P-after_2y', 'EDSS-after_2y', 'NRELAP', 'CESEV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RACE</th>\n",
       "      <th>CONTINENT</th>\n",
       "      <th>MHDIAGN</th>\n",
       "      <th>CARDIO</th>\n",
       "      <th>URINARY</th>\n",
       "      <th>MUSCKELET</th>\n",
       "      <th>FATIGUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NORTH AMERICA</td>\n",
       "      <td>SPMS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.0</td>\n",
       "      <td>M</td>\n",
       "      <td>NON-WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PPMS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.0</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NORTH AMERICA</td>\n",
       "      <td>SPMS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>EUROPE</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>46.0</td>\n",
       "      <td>M</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>OCEANIA</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>18.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>38.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>40.0</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PPMS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>38.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RRMS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2465 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AGE SEX       RACE      CONTINENT MHDIAGN  CARDIO  URINARY  MUSCKELET  \\\n",
       "0     46.0   F        NaN            NaN    RRMS       0        0          0   \n",
       "1      NaN   M      WHITE  NORTH AMERICA    SPMS       1        1          0   \n",
       "2     44.0   M  NON-WHITE            NaN    PPMS       1        1          0   \n",
       "3     60.0   M      WHITE  NORTH AMERICA    SPMS       1        1          1   \n",
       "4     28.0   F      WHITE         EUROPE    RRMS       1        1          0   \n",
       "...    ...  ..        ...            ...     ...     ...      ...        ...   \n",
       "2460  46.0   M      WHITE        OCEANIA    RRMS       1        1          0   \n",
       "2461  18.0   F        NaN            NaN    RRMS       0        0          0   \n",
       "2462  38.0   F        NaN            NaN    RRMS       0        0          0   \n",
       "2463  40.0   F      WHITE            NaN    PPMS       0        1          0   \n",
       "2464  38.0   F        NaN            NaN    RRMS       0        0          0   \n",
       "\n",
       "      FATIGUE  \n",
       "0           0  \n",
       "1           1  \n",
       "2           0  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "2460        1  \n",
       "2461        0  \n",
       "2462        0  \n",
       "2463        1  \n",
       "2464        0  \n",
       "\n",
       "[2465 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract targets\n",
    "targets = data[variables]\n",
    "\n",
    "# Extract features by dropping the target columns\n",
    "#features = data.drop(variables, axis=1)\n",
    "\n",
    "columns_to_keep = ['AGE', 'SEX', 'RACE', 'CONTINENT', 'MHDIAGN', 'CARDIO', 'URINARY', 'MUSCKELET', 'FATIGUE']\n",
    "                    #'PASAT_2s-before', 'PASAT_3s-before', 'SDMT-before', 'BDI-before', 'T-before','P-before','N-before']\n",
    "# removed: 'NHPT-before', 'T25FW-before', 'SLEC_before','SES_before','EDSS-before', 'KFSS_M-before', 'KFSS_P-before', 'M_R36-SF12-before', \n",
    "# 'P_R36-SF12-before', 'R36-SF12-before_Ind',\n",
    "\n",
    "features = data[columns_to_keep]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>CARDIO</th>\n",
       "      <th>URINARY</th>\n",
       "      <th>MUSCKELET</th>\n",
       "      <th>FATIGUE</th>\n",
       "      <th>SEX_F</th>\n",
       "      <th>SEX_M</th>\n",
       "      <th>RACE_NON-WHITE</th>\n",
       "      <th>RACE_WHITE</th>\n",
       "      <th>CONTINENT_ASIA</th>\n",
       "      <th>CONTINENT_EURASIA</th>\n",
       "      <th>CONTINENT_EUROPE</th>\n",
       "      <th>CONTINENT_NORTH AMERICA</th>\n",
       "      <th>CONTINENT_OCEANIA</th>\n",
       "      <th>CONTINENT_SOUTH AMERICA</th>\n",
       "      <th>MHDIAGN_PPMS</th>\n",
       "      <th>MHDIAGN_RRMS</th>\n",
       "      <th>MHDIAGN_SPMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AGE  CARDIO  URINARY  MUSCKELET  FATIGUE  SEX_F  SEX_M  RACE_NON-WHITE  \\\n",
       "0  46.0       0        0          0        0      1      0               0   \n",
       "1   NaN       1        1          0        1      0      1               0   \n",
       "2  44.0       1        1          0        0      0      1               1   \n",
       "3  60.0       1        1          1        1      0      1               0   \n",
       "4  28.0       1        1          0        1      1      0               0   \n",
       "\n",
       "   RACE_WHITE  CONTINENT_ASIA  CONTINENT_EURASIA  CONTINENT_EUROPE  \\\n",
       "0           0               0                  0                 0   \n",
       "1           1               0                  0                 0   \n",
       "2           0               0                  0                 0   \n",
       "3           1               0                  0                 0   \n",
       "4           1               0                  0                 1   \n",
       "\n",
       "   CONTINENT_NORTH AMERICA  CONTINENT_OCEANIA  CONTINENT_SOUTH AMERICA  \\\n",
       "0                        0                  0                        0   \n",
       "1                        1                  0                        0   \n",
       "2                        0                  0                        0   \n",
       "3                        1                  0                        0   \n",
       "4                        0                  0                        0   \n",
       "\n",
       "   MHDIAGN_PPMS  MHDIAGN_RRMS  MHDIAGN_SPMS  \n",
       "0             0             1             0  \n",
       "1             0             0             1  \n",
       "2             1             0             0  \n",
       "3             0             0             1  \n",
       "4             0             1             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_columns = features.select_dtypes(include=['object'])\n",
    "features = pd.get_dummies(features, columns=object_columns.columns, dtype=int)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFSS_M-2y           float64\n",
       "KFSS_P-2y           float64\n",
       "EDSS-2y             float64\n",
       "T25FW-2y            float64\n",
       "NHPT-2y             float64\n",
       "P_R36-SF12-after    float64\n",
       "M_R36-SF12-after    float64\n",
       "SES_after           float64\n",
       "SLEC_after          float64\n",
       "KFSS_M-after_2y     float64\n",
       "KFSS_P-after_2y     float64\n",
       "EDSS-after_2y       float64\n",
       "NRELAP               object\n",
       "CESEV                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random state for reproducibility\n",
    "random_state = 42\n",
    "N_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV Fold\n",
       "4.0    510\n",
       "3.0    502\n",
       "0.0    500\n",
       "1.0    495\n",
       "2.0    458\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate CV folds\n",
    "cv=missingness_and_categorical_stratified_cv(targets, N_FOLDS, random_state)\n",
    "cv = cv.to_frame(name=\"CV Fold\")\n",
    "\n",
    "features_cv = pd.merge(features, pd.DataFrame(cv), left_index=True, right_index=True)\n",
    "targets_cv = pd.merge(targets, pd.DataFrame(cv), left_index=True, right_index=True)\n",
    "\n",
    "features_cv['CV Fold'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it a problem that not all folds have the exact same number?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_mean_squared_error(true, pred, train):\n",
    "    num = mse(true, pred)\n",
    "    mean_value = np.mean(train)\n",
    "    mean = np.full_like(true, mean_value)\n",
    "    den = mse(true, mean)\n",
    "    nmse_loss = num/den\n",
    "    #rrmse_loss = np.sqrt(squared_error)\n",
    "    return nmse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with evaluating on CV Fold 1\n",
      "Done with evaluating on CV Fold 2\n",
      "Done with evaluating on CV Fold 3\n",
      "Done with evaluating on CV Fold 4\n",
      "Done with evaluating on CV Fold 5\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "y_train_list = []\n",
    "y_pred_prob_list = []\n",
    "yi_test_dummies_list = []\n",
    "yi_train_dummies_list = []\n",
    "\n",
    "for i in range(0, N_FOLDS): \n",
    "    Xi_train = features_cv[features_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    Xi_test = features_cv[features_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_train = targets_cv[targets_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_test = targets_cv[targets_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    y_test_list.append(pd.DataFrame(yi_test, columns=yi_test.columns, index=yi_test.index))\n",
    "    y_train_list.append(pd.DataFrame(yi_train, columns=yi_train.columns, index=yi_train.index))\n",
    "\n",
    "    # One hot encode categorical targets of test set to be able to compute brier score\n",
    "    subset_yi_test = yi_test.select_dtypes(include=['object'])\n",
    "    yi_test_dummies = pd.get_dummies(subset_yi_test, columns=subset_yi_test.columns, dtype=int)\n",
    "    subset_yi_train = yi_train.select_dtypes(include=['object'])\n",
    "    yi_train_dummies = pd.get_dummies(subset_yi_train, columns=subset_yi_train.columns, dtype=int)\n",
    "    \n",
    "\n",
    "    chain = Chain(\n",
    "        model_reg=RandomForestRegressor(random_state=random_state),\n",
    "        model_clf=RandomForestClassifier(random_state=random_state),\n",
    "        propagate=False, #RUN LOCAL MODELS \n",
    "    )\n",
    "    chain.fit(Xi_train, yi_train, target_types=None) #[\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"clf\",\"clf\"]\n",
    "    y_pred = chain.predict(Xi_test)\n",
    "    y_pred_prob = chain.predict_proba(Xi_test)\n",
    "    y_pred_list.append(y_pred)\n",
    "    y_pred_prob_list.append(y_pred_prob)\n",
    "    yi_test_dummies_list.append(yi_test_dummies)\n",
    "    yi_train_dummies_list.append(yi_train_dummies)\n",
    "    \n",
    "    print(\"Done with evaluating on CV Fold {}\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi_train_dummies_avg = []\n",
    "i=0\n",
    "# Calculate the percentage of 1s in each column\n",
    "for yi_train_dummies_fold in yi_train_dummies_list:\n",
    "\n",
    "    percentages = yi_train_dummies_fold.sum() / len(yi_train_dummies_fold)\n",
    "\n",
    "    yi_train_dummies_avg_fold = pd.DataFrame(0, index=yi_test_dummies_list[i].index, columns=yi_train_dummies_fold.columns)\n",
    "\n",
    "    # Replace values in each column with the corresponding percentage\n",
    "    for col in yi_train_dummies_avg_fold.columns:\n",
    "        yi_train_dummies_avg_fold[col] = yi_train_dummies_avg_fold[col].apply(lambda x: percentages[col])\n",
    "    \n",
    "    i += 1\n",
    "    yi_train_dummies_avg.append(yi_train_dummies_avg_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_dfs = []\n",
    "\n",
    "# Iterate over each pair of arrays\n",
    "for j, fold in enumerate(y_pred_prob_list):\n",
    "    dfs = []\n",
    "    len_array = 0\n",
    "    \n",
    "    for i, array in enumerate(fold):\n",
    "        # Convert array to DataFrame\n",
    "        col = yi_test_dummies_list[j].columns[len_array:len_array+len(array[0])]\n",
    "        df = pd.DataFrame(array, columns=col, index=yi_test_dummies_list[j].index)\n",
    "        dfs.append(df)\n",
    "        len_array += len(array[0])\n",
    "    \n",
    "    # Concatenate DataFrames\n",
    "    concatenated_df = pd.concat(dfs, axis=1)\n",
    "    concatenated_dfs.append(concatenated_df)\n",
    "\n",
    "# Now you should have a list of concatenated DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store scores\n",
    "scores_with_std = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for variable_name in variables_cat: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Compute scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][variable_name] \n",
    "        y_prob = concatenated_dfs[fold_index][variable_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][variable_name] \n",
    "        \n",
    "        # Check if the target variable is numerical or categorical\n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "\n",
    "        normalized_brier= brier_score/brier_baseline\n",
    "                  \n",
    "        variable_scores.append(normalized_brier)\n",
    "    \n",
    "    # Compute the average score for the variable across all folds\n",
    "    variable_avg_score = np.mean(variable_scores)\n",
    "    \n",
    "    # Compute the standard deviation for the variable across all folds\n",
    "    variable_std_score = np.std(variable_scores)\n",
    "    \n",
    "    scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "#print(\"Normalized Brier scores for each level:\")\n",
    "#for variable_name, avg_score, std_score in scores_with_std:\n",
    "#    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store column sums\n",
    "column_sums = {}\n",
    "\n",
    "# Iterate over each dataframe in reorganized_dummy_list_first\n",
    "for df in yi_test_dummies_list:\n",
    "    # Iterate over each column in the dataframe\n",
    "    for column in df.columns:\n",
    "        # Sum occurrences of 1s in the column and update column_sums\n",
    "        column_sum = df[column].sum()\n",
    "        column_sums[column] = column_sums.get(column, 0) + column_sum\n",
    "\n",
    "# Create a dataframe from the column sums\n",
    "total_counts_df = pd.DataFrame(list(column_sums.items()), columns=['Name', 'Total'])\n",
    "#total_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted avg for NRELAP: 1.07\n",
      "Weighted avg for CESEV: 1.96\n"
     ]
    }
   ],
   "source": [
    "# Calculate the weighted sum for NRELAP\n",
    "nrelap_weighted_sum = 0\n",
    "total_count = 0\n",
    "for level_name, avg_score, _ in scores_with_std:\n",
    "    if level_name.startswith(\"NRELAP\"):\n",
    "        count = total_counts_df.loc[total_counts_df['Name'] == level_name, 'Total'].values[0]\n",
    "        nrelap_weighted_sum += avg_score * count\n",
    "        total_count += count\n",
    "nrelap_weighted_avg = nrelap_weighted_sum / total_count\n",
    "\n",
    "# Calculate the weighted sum for CESEV\n",
    "cesev_weighted_sum = 0\n",
    "total_count = 0\n",
    "for level_name, avg_score, _ in scores_with_std:\n",
    "    if level_name.startswith(\"CESEV\"):\n",
    "        count = total_counts_df.loc[total_counts_df['Name'] == level_name, 'Total'].values[0]\n",
    "        cesev_weighted_sum += avg_score * count\n",
    "        total_count += count\n",
    "cesev_weighted_avg = cesev_weighted_sum / total_count\n",
    "\n",
    "# Print the results\n",
    "print(f\"Weighted avg for NRELAP: {nrelap_weighted_avg:.2f}\")\n",
    "print(f\"Weighted avg for CESEV: {cesev_weighted_avg:.2f}\")\n",
    "\n",
    "cat_normalized_brier= [nrelap_weighted_avg, cesev_weighted_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Brier scores for each categorical variable:\n",
      "NRELAP: 1.15 (± 0.11)\n",
      "CESEV: 1.80 (± 0.36)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store scores\n",
    "scores_with_std = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "\n",
    "# Create a dictionary to store the scores for variables with the same letters before the '_'\n",
    "variable_scores_dict = {}\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for level_name in variables_cat: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Compute scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][level_name] \n",
    "        y_prob = concatenated_dfs[fold_index][level_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][level_name] \n",
    "        \n",
    "        # Compute the Brier score and the normalized Brier score\n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "        normalized_brier = brier_score / brier_baseline\n",
    "\n",
    "        # Append the normalized Brier score to the variable scores list\n",
    "        variable_scores.append(normalized_brier)\n",
    "    \n",
    "    # Check if the variable name has letters before the '_'\n",
    "    prefix = level_name.split('_')[0]\n",
    "    \n",
    "    # Add the normalized Brier scores to the dictionary based on the prefix\n",
    "    if prefix in variable_scores_dict:\n",
    "        variable_scores_dict[prefix].extend(variable_scores)\n",
    "    else:\n",
    "        variable_scores_dict[prefix] = variable_scores\n",
    "\n",
    "# Compute the average and standard deviation of normalized Brier score for each prefix\n",
    "for prefix, scores in variable_scores_dict.items():\n",
    "    avg_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    scores_with_std.append((prefix, avg_score, std_score))\n",
    "\n",
    "#cat_normalized_brier = []\n",
    "cat_std_brier = []\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "print(\"Normalized Brier scores for each categorical variable:\")\n",
    "for prefix, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{prefix}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    #cat_normalized_brier.append(avg_score)\n",
    "    cat_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cv = []\n",
    "y_pred_cv = []\n",
    "\n",
    "for j in range(len(y_test_list)):  # 5\n",
    "    y_test_targ = []\n",
    "    y_pred_targ = []\n",
    "    nvar=y_test_list[0].shape[1]\n",
    "\n",
    "    for i in range(0, nvar):  # or (1, 5)\n",
    "        missing_rows_mask = y_test_list[j].iloc[:, i].isna()\n",
    "        y_test = y_test_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        y_pred = y_pred_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        \n",
    "        y_test_targ.append(y_test)\n",
    "        y_pred_targ.append(y_pred)\n",
    "    \n",
    "    y_test_cv.append(y_test_targ)\n",
    "    y_pred_cv.append(y_pred_targ)\n",
    "# y_test_cv[fold][outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for each outcome (local):\n",
      "KFSS_M-2y: 0.71 (± 0.07)\n",
      "KFSS_P-2y: 0.87 (± 0.11)\n",
      "EDSS-2y: 0.52 (± 0.03)\n",
      "T25FW-2y: 1.22 (± 0.22)\n",
      "NHPT-2y: 1.33 (± 0.33)\n",
      "P_R36-SF12-after: 0.86 (± 0.02)\n",
      "M_R36-SF12-after: 1.09 (± 0.03)\n",
      "SES_after: 1.07 (± 0.10)\n",
      "SLEC_after: 1.12 (± 0.07)\n",
      "KFSS_M-after_2y: 0.75 (± 0.05)\n",
      "KFSS_P-after_2y: 1.09 (± 0.22)\n",
      "EDSS-after_2y: 0.60 (± 0.06)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store scores\n",
    "scores_with_std = []\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for variable_name in variables: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Check if the target variable is numerical or categorical\n",
    "    if y_test_cv[0][variables.index(variable_name)].dtype.kind in 'bifc':\n",
    "        # Compute scores for the variable across all folds\n",
    "        for fold_index in range(len(y_test_cv)):\n",
    "            y_test = y_test_cv[fold_index][variables.index(variable_name)] \n",
    "            y_pred = y_pred_cv[fold_index][variables.index(variable_name)] \n",
    "            y_train = y_train_list[fold_index][variable_name]\n",
    "\n",
    "            score = normalized_mean_squared_error(y_test, y_pred, y_train)\n",
    "            variable_scores.append(score)\n",
    "        \n",
    "        # Compute the average score for the variable across all folds\n",
    "        variable_avg_score = np.mean(variable_scores)\n",
    "        \n",
    "        # Compute the standard deviation for the variable across all folds\n",
    "        variable_std_score = np.std(variable_scores)\n",
    "        \n",
    "        # Append the tuple with three elements to the scores_with_std list\n",
    "        scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "num_normalized_brier=[]\n",
    "num_std_brier=[]\n",
    "\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "print(\"Scores for each outcome (local):\")\n",
    "for variable_name, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    num_normalized_brier.append(avg_score)\n",
    "    num_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7144871 , 0.86754384, 0.51699737, 1.22284979, 1.3320739 ,\n",
       "       0.86166666, 1.08575278, 1.06705598, 1.12374131, 0.75011666,\n",
       "       1.08720878, 0.60046788, 1.0726391 , 1.95922851])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_normalized_brier = np.concatenate((num_normalized_brier, cat_normalized_brier))\n",
    "combined_normalized_brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07249766, 0.11373715, 0.02831994, 0.2236021 , 0.32955757,\n",
       "       0.02470746, 0.0342752 , 0.10139115, 0.07338219, 0.05215299,\n",
       "       0.2228481 , 0.06095894, 0.11462421, 0.36300708])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_std_brier = np.concatenate((num_std_brier, cat_std_brier))\n",
    "combined_std_brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized Brier score: 1.018702118992026\n"
     ]
    }
   ],
   "source": [
    "# Compute the average\n",
    "average_normalized_brier = np.mean(combined_normalized_brier)\n",
    "print(\"Average normalized Brier score:\", average_normalized_brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of the combined average: 0.6293334323107214\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have an array of individual values to average\n",
    "values_to_average = combined_normalized_brier  # Your individual values here\n",
    "\n",
    "# Assuming you have an array of standard deviations corresponding to the individual values\n",
    "individual_std_devs = combined_std_brier  # Your array of individual standard deviations here\n",
    "\n",
    "# Step 1: Calculate the combined average\n",
    "combined_average = np.mean(values_to_average)\n",
    "\n",
    "# Step 2: Calculate the standard error of the mean (SEM)\n",
    "sem = np.sqrt(np.sum(individual_std_devs**2) / len(values_to_average))\n",
    "\n",
    "# Step 3: Calculate the standard deviation of the combined average\n",
    "combined_std_dev = sem * np.sqrt(len(values_to_average))\n",
    "\n",
    "# Now you have the standard deviation of the combined average\n",
    "print(\"Standard deviation of the combined average:\", combined_std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with evaluating on CV Fold 1\n",
      "Done with evaluating on CV Fold 2\n",
      "Done with evaluating on CV Fold 3\n",
      "Done with evaluating on CV Fold 4\n",
      "Done with evaluating on CV Fold 5\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "y_train_list = []\n",
    "y_pred_prob_list = []\n",
    "yi_test_dummies_list = []\n",
    "yi_train_dummies_list = []\n",
    "\n",
    "for i in range(0, N_FOLDS): \n",
    "    Xi_train = features_cv[features_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    Xi_test = features_cv[features_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_train = targets_cv[targets_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_test = targets_cv[targets_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    y_test_list.append(pd.DataFrame(yi_test, columns=yi_test.columns, index=yi_test.index))\n",
    "    y_train_list.append(pd.DataFrame(yi_train, columns=yi_train.columns, index=yi_train.index))\n",
    "\n",
    "\n",
    "    # One hot encode categorical targets of test set to be able to compute brier score\n",
    "    subset_yi_test = yi_test.select_dtypes(include=['object'])\n",
    "    yi_test_dummies = pd.get_dummies(subset_yi_test, columns=subset_yi_test.columns, dtype=int)\n",
    "    subset_yi_train = yi_train.select_dtypes(include=['object'])\n",
    "    yi_train_dummies = pd.get_dummies(subset_yi_train, columns=subset_yi_train.columns, dtype=int)\n",
    "    \n",
    "\n",
    "    chain = Chain(\n",
    "        model_reg=RandomForestRegressor(random_state=random_state),\n",
    "        model_clf=RandomForestClassifier(random_state=random_state),\n",
    "        propagate=\"pred\", #RUN MODELS IN A CHAIN\n",
    "    )\n",
    "\n",
    "\n",
    "    chain.fit(Xi_train, yi_train, target_types=None) #[\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"clf\",\"clf\"]\n",
    "    y_pred = chain.predict(Xi_test)\n",
    "    y_pred_prob = chain.predict_proba(Xi_test)\n",
    "    y_pred_list.append(y_pred)\n",
    "    y_pred_prob_list.append(y_pred_prob)\n",
    "    yi_test_dummies_list.append(yi_test_dummies)\n",
    "    yi_train_dummies_list.append(yi_train_dummies)\n",
    "    \n",
    "    print(\"Done with evaluating on CV Fold {}\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi_train_dummies_avg = []\n",
    "i=0\n",
    "# Calculate the percentage of 1s in each column\n",
    "for yi_train_dummies_fold in yi_train_dummies_list:\n",
    "\n",
    "    percentages = yi_train_dummies_fold.sum() / len(yi_train_dummies_fold)\n",
    "\n",
    "    yi_train_dummies_avg_fold = pd.DataFrame(0, index=yi_test_dummies_list[i].index, columns=yi_train_dummies_fold.columns)\n",
    "\n",
    "    # Replace values in each column with the corresponding percentage\n",
    "    for col in yi_train_dummies_avg_fold.columns:\n",
    "        yi_train_dummies_avg_fold[col] = yi_train_dummies_avg_fold[col].apply(lambda x: percentages[col])\n",
    "    \n",
    "    i += 1\n",
    "    yi_train_dummies_avg.append(yi_train_dummies_avg_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the concatenated DataFrames\n",
    "concatenated_dfs = []\n",
    "\n",
    "# Iterate over each pair of arrays\n",
    "for j, fold in enumerate(y_pred_prob_list):\n",
    "    dfs = []\n",
    "    len_array = 0\n",
    "    \n",
    "    for i, array in enumerate(fold):\n",
    "        # Convert array to DataFrame\n",
    "        col = yi_test_dummies_list[j].columns[len_array:len_array+len(array[0])]\n",
    "        df = pd.DataFrame(array, columns=col, index=yi_test_dummies_list[j].index)\n",
    "        dfs.append(df)\n",
    "        len_array += len(array[0])\n",
    "    \n",
    "    # Concatenate DataFrames\n",
    "    concatenated_df = pd.concat(dfs, axis=1)\n",
    "    concatenated_dfs.append(concatenated_df)\n",
    "\n",
    "# Now you should have a list of concatenated DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store scores\n",
    "scores_with_std = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for variable_name in variables_cat: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Compute scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][variable_name] \n",
    "        y_prob = concatenated_dfs[fold_index][variable_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][variable_name] \n",
    "        \n",
    "        # Check if the target variable is numerical or categorical\n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "\n",
    "        normalized_brier= brier_score/brier_baseline\n",
    "                  \n",
    "        variable_scores.append(normalized_brier)\n",
    "    \n",
    "    # Compute the average score for the variable across all folds\n",
    "    variable_avg_score = np.mean(variable_scores)\n",
    "    \n",
    "    # Compute the standard deviation for the variable across all folds\n",
    "    variable_std_score = np.std(variable_scores)\n",
    "    \n",
    "    scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "#print(\"Normalized Brier scores for each level:\")\n",
    "#for variable_name, avg_score, std_score in scores_with_std:\n",
    "#    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store column sums\n",
    "column_sums = {}\n",
    "\n",
    "# Iterate over each dataframe in reorganized_dummy_list_first\n",
    "for df in yi_test_dummies_list:\n",
    "    # Iterate over each column in the dataframe\n",
    "    for column in df.columns:\n",
    "        # Sum occurrences of 1s in the column and update column_sums\n",
    "        column_sum = df[column].sum()\n",
    "        column_sums[column] = column_sums.get(column, 0) + column_sum\n",
    "\n",
    "# Create a dataframe from the column sums\n",
    "total_counts_df = pd.DataFrame(list(column_sums.items()), columns=['Name', 'Total'])\n",
    "#total_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted avg for NRELAP: 1.02\n",
      "Weighted avg for CESEV: 1.78\n"
     ]
    }
   ],
   "source": [
    "# Calculate the weighted sum for NRELAP\n",
    "nrelap_weighted_sum = 0\n",
    "total_count = 0\n",
    "for level_name, avg_score, _ in scores_with_std:\n",
    "    if level_name.startswith(\"NRELAP\"):\n",
    "        count = total_counts_df.loc[total_counts_df['Name'] == level_name, 'Total'].values[0]\n",
    "        nrelap_weighted_sum += avg_score * count\n",
    "        total_count += count\n",
    "nrelap_weighted_avg = nrelap_weighted_sum / total_count\n",
    "\n",
    "# Calculate the weighted sum for CESEV\n",
    "cesev_weighted_sum = 0\n",
    "total_count = 0\n",
    "for level_name, avg_score, _ in scores_with_std:\n",
    "    if level_name.startswith(\"CESEV\"):\n",
    "        count = total_counts_df.loc[total_counts_df['Name'] == level_name, 'Total'].values[0]\n",
    "        cesev_weighted_sum += avg_score * count\n",
    "        total_count += count\n",
    "cesev_weighted_avg = cesev_weighted_sum / total_count\n",
    "\n",
    "# Print the results\n",
    "print(f\"Weighted avg for NRELAP: {nrelap_weighted_avg:.2f}\")\n",
    "print(f\"Weighted avg for CESEV: {cesev_weighted_avg:.2f}\")\n",
    "\n",
    "cat_normalized_brier = [nrelap_weighted_avg, cesev_weighted_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Brier scores for each categorical variable:\n",
      "NRELAP: 1.09 (± 0.08)\n",
      "CESEV: 1.66 (± 0.31)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store scores\n",
    "scores_with_std = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "\n",
    "# Create a dictionary to store the scores for variables with the same letters before the '_'\n",
    "variable_scores_dict = {}\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for level_name in variables_cat: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Compute scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][level_name] \n",
    "        y_prob = concatenated_dfs[fold_index][level_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][level_name] \n",
    "        \n",
    "        # Compute the Brier score and the normalized Brier score\n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "        normalized_brier = brier_score / brier_baseline\n",
    "\n",
    "        # Append the normalized Brier score to the variable scores list\n",
    "        variable_scores.append(normalized_brier)\n",
    "    \n",
    "    # Check if the variable name has letters before the '_'\n",
    "    prefix = level_name.split('_')[0]\n",
    "    \n",
    "    # Add the normalized Brier scores to the dictionary based on the prefix\n",
    "    if prefix in variable_scores_dict:\n",
    "        variable_scores_dict[prefix].extend(variable_scores)\n",
    "    else:\n",
    "        variable_scores_dict[prefix] = variable_scores\n",
    "\n",
    "# Compute the average and standard deviation of normalized Brier score for each prefix\n",
    "for prefix, scores in variable_scores_dict.items():\n",
    "    avg_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    scores_with_std.append((prefix, avg_score, std_score))\n",
    "\n",
    "#cat_normalized_brier = []\n",
    "cat_std_brier = []\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "print(\"Normalized Brier scores for each categorical variable:\")\n",
    "for prefix, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{prefix}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    #cat_normalized_brier.append(avg_score)\n",
    "    cat_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for each outcome (chain - propagate predictions):\n",
      "KFSS_M-2y: 0.71 (± 0.07)\n",
      "KFSS_P-2y: 0.82 (± 0.09)\n",
      "EDSS-2y: 0.54 (± 0.04)\n",
      "T25FW-2y: 1.16 (± 0.12)\n",
      "NHPT-2y: 1.16 (± 0.27)\n",
      "P_R36-SF12-after: 0.90 (± 0.04)\n",
      "M_R36-SF12-after: 1.15 (± 0.08)\n",
      "SES_after: 1.07 (± 0.09)\n",
      "SLEC_after: 1.17 (± 0.10)\n",
      "KFSS_M-after_2y: 0.80 (± 0.03)\n",
      "KFSS_P-after_2y: 1.14 (± 0.15)\n",
      "EDSS-after_2y: 0.68 (± 0.08)\n"
     ]
    }
   ],
   "source": [
    "y_test_cv = []\n",
    "y_pred_cv = []\n",
    "\n",
    "for j in range(len(y_test_list)):  # 5\n",
    "    y_test_targ = []\n",
    "    y_pred_targ = []\n",
    "    nvar=y_test_list[0].shape[1]\n",
    "\n",
    "    for i in range(0, nvar):  # or (1, 5)\n",
    "        missing_rows_mask = y_test_list[j].iloc[:, i].isna()\n",
    "        y_test = y_test_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        y_pred = y_pred_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        \n",
    "        y_test_targ.append(y_test)\n",
    "        y_pred_targ.append(y_pred)\n",
    "    \n",
    "    y_test_cv.append(y_test_targ)\n",
    "    y_pred_cv.append(y_pred_targ)\n",
    "# y_test_cv[fold][outcome]\n",
    "\n",
    "\n",
    "# Initialize a list to store scores\n",
    "scores_with_std = []\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for variable_name in variables: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Check if the target variable is numerical or categorical\n",
    "    if y_test_cv[0][variables.index(variable_name)].dtype.kind in 'bifc':\n",
    "        # Compute scores for the variable across all folds\n",
    "        for fold_index in range(len(y_test_cv)):\n",
    "            y_test = y_test_cv[fold_index][variables.index(variable_name)] \n",
    "            y_pred = y_pred_cv[fold_index][variables.index(variable_name)] \n",
    "            y_train = y_train_list[fold_index][variable_name]\n",
    "\n",
    "            score = normalized_mean_squared_error(y_test, y_pred, y_train)\n",
    "            variable_scores.append(score)\n",
    "        \n",
    "        # Compute the average score for the variable across all folds\n",
    "        variable_avg_score = np.mean(variable_scores)\n",
    "        \n",
    "        # Compute the standard deviation for the variable across all folds\n",
    "        variable_std_score = np.std(variable_scores)\n",
    "        \n",
    "        # Append the tuple with three elements to the scores_with_std list\n",
    "        scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "num_normalized_brier=[]\n",
    "num_std_brier=[]\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "print(\"Scores for each outcome (chain - propagate predictions):\")\n",
    "for variable_name, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    num_normalized_brier.append(avg_score)\n",
    "    num_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7144871 , 0.82008945, 0.53679771, 1.15815499, 1.163118  ,\n",
       "       0.8959963 , 1.1547296 , 1.06726975, 1.1652273 , 0.79620393,\n",
       "       1.13779282, 0.68285029, 1.02286867, 1.78158394])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_normalized_brier = np.concatenate((num_normalized_brier, cat_normalized_brier))\n",
    "combined_normalized_brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07249766, 0.09334165, 0.03613727, 0.11660765, 0.27366932,\n",
       "       0.04361651, 0.07540907, 0.08562591, 0.09624444, 0.03210696,\n",
       "       0.15375922, 0.07552454, 0.07816503, 0.30706341])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_std_brier = np.concatenate((num_std_brier, cat_std_brier))\n",
    "combined_std_brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized Brier score: 1.0069407032933622\n"
     ]
    }
   ],
   "source": [
    "# Compute the average\n",
    "average_normalized_brier = np.mean(combined_normalized_brier)\n",
    "print(\"Average normalized Brier score:\", average_normalized_brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of the combined average: 0.5086500270994773\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have an array of individual values to average\n",
    "values_to_average = combined_normalized_brier  # Your individual values here\n",
    "\n",
    "# Assuming you have an array of standard deviations corresponding to the individual values\n",
    "individual_std_devs = combined_std_brier  # Your array of individual standard deviations here\n",
    "\n",
    "# Step 1: Calculate the combined average\n",
    "combined_average = np.mean(values_to_average)\n",
    "\n",
    "# Step 2: Calculate the standard error of the mean (SEM)\n",
    "sem = np.sqrt(np.sum(individual_std_devs**2) / len(values_to_average))\n",
    "\n",
    "# Step 3: Calculate the standard deviation of the combined average\n",
    "combined_std_dev = sem * np.sqrt(len(values_to_average))\n",
    "\n",
    "# Now you have the standard deviation of the combined average\n",
    "print(\"Standard deviation of the combined average:\", combined_std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagate true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with evaluating on CV Fold 1\n",
      "Done with evaluating on CV Fold 2\n",
      "Done with evaluating on CV Fold 3\n",
      "Done with evaluating on CV Fold 4\n",
      "Done with evaluating on CV Fold 5\n"
     ]
    }
   ],
   "source": [
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "y_train_list = []\n",
    "y_pred_prob_list = []\n",
    "yi_test_dummies_list = []\n",
    "yi_train_dummies_list = []\n",
    "\n",
    "for i in range(0, N_FOLDS): \n",
    "    Xi_train = features_cv[features_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    Xi_test = features_cv[features_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_train = targets_cv[targets_cv['CV Fold'] != i].drop([\"CV Fold\"], axis=1)\n",
    "    yi_test = targets_cv[targets_cv['CV Fold'] == i].drop([\"CV Fold\"], axis=1)\n",
    "    y_test_list.append(pd.DataFrame(yi_test, columns=yi_test.columns, index=yi_test.index))\n",
    "    y_train_list.append(pd.DataFrame(yi_train, columns=yi_train.columns, index=yi_train.index))\n",
    "\n",
    "    # One hot encode categorical targets of test set to be able to compute brier score\n",
    "    subset_yi_test = yi_test.select_dtypes(include=['object'])\n",
    "    yi_test_dummies = pd.get_dummies(subset_yi_test, columns=subset_yi_test.columns, dtype=int)\n",
    "    subset_yi_train = yi_train.select_dtypes(include=['object'])\n",
    "    yi_train_dummies = pd.get_dummies(subset_yi_train, columns=subset_yi_train.columns, dtype=int)\n",
    "    \n",
    "\n",
    "    chain = Chain(\n",
    "        model_reg=RandomForestRegressor(random_state=random_state),\n",
    "        model_clf=RandomForestClassifier(random_state=random_state),\n",
    "        propagate=\"true\", #RUN MODELS IN A CHAIN\n",
    "    )\n",
    "\n",
    "\n",
    "    chain.fit(Xi_train, yi_train, target_types=None) #[\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"reg\",\"clf\",\"clf\"]\n",
    "    y_pred = chain.predict(Xi_test)\n",
    "    y_pred_prob = chain.predict_proba(Xi_test)\n",
    "    y_pred_list.append(y_pred)\n",
    "    y_pred_prob_list.append(y_pred_prob)\n",
    "    yi_test_dummies_list.append(yi_test_dummies)\n",
    "    yi_train_dummies_list.append(yi_train_dummies)\n",
    "    \n",
    "    print(\"Done with evaluating on CV Fold {}\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi_train_dummies_avg = []\n",
    "i=0\n",
    "# Calculate the percentage of 1s in each column\n",
    "for yi_train_dummies_fold in yi_train_dummies_list:\n",
    "\n",
    "    percentages = yi_train_dummies_fold.sum() / len(yi_train_dummies_fold)\n",
    "\n",
    "    yi_train_dummies_avg_fold = pd.DataFrame(0, index=yi_test_dummies_list[i].index, columns=yi_train_dummies_fold.columns)\n",
    "\n",
    "    # Replace values in each column with the corresponding percentage\n",
    "    for col in yi_train_dummies_avg_fold.columns:\n",
    "        yi_train_dummies_avg_fold[col] = yi_train_dummies_avg_fold[col].apply(lambda x: percentages[col])\n",
    "    \n",
    "    i += 1\n",
    "    yi_train_dummies_avg.append(yi_train_dummies_avg_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the concatenated DataFrames\n",
    "concatenated_dfs = []\n",
    "\n",
    "# Iterate over each pair of arrays\n",
    "for j, fold in enumerate(y_pred_prob_list):\n",
    "    dfs = []\n",
    "    len_array = 0\n",
    "    \n",
    "    for i, array in enumerate(fold):\n",
    "        # Convert array to DataFrame\n",
    "        col = yi_test_dummies_list[j].columns[len_array:len_array+len(array[0])]\n",
    "        df = pd.DataFrame(array, columns=col, index=yi_test_dummies_list[j].index)\n",
    "        dfs.append(df)\n",
    "        len_array += len(array[0])\n",
    "    \n",
    "    # Concatenate DataFrames\n",
    "    concatenated_df = pd.concat(dfs, axis=1)\n",
    "    concatenated_dfs.append(concatenated_df)\n",
    "\n",
    "# Now you should have a list of concatenated DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store scores\n",
    "scores_with_std = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for variable_name in variables_cat: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Compute scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][variable_name] \n",
    "        y_prob = concatenated_dfs[fold_index][variable_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][variable_name] \n",
    "        \n",
    "        # Check if the target variable is numerical or categorical\n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "\n",
    "        normalized_brier= brier_score/brier_baseline\n",
    "                  \n",
    "        variable_scores.append(normalized_brier)\n",
    "    \n",
    "    # Compute the average score for the variable across all folds\n",
    "    variable_avg_score = np.mean(variable_scores)\n",
    "    \n",
    "    # Compute the standard deviation for the variable across all folds\n",
    "    variable_std_score = np.std(variable_scores)\n",
    "    \n",
    "    scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "#print(\"Normalized Brier scores for each level:\")\n",
    "#for variable_name, avg_score, std_score in scores_with_std:\n",
    "#    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store column sums\n",
    "column_sums = {}\n",
    "\n",
    "# Iterate over each dataframe in reorganized_dummy_list_first\n",
    "for df in yi_test_dummies_list:\n",
    "    # Iterate over each column in the dataframe\n",
    "    for column in df.columns:\n",
    "        # Sum occurrences of 1s in the column and update column_sums\n",
    "        column_sum = df[column].sum()\n",
    "        column_sums[column] = column_sums.get(column, 0) + column_sum\n",
    "\n",
    "# Create a dataframe from the column sums\n",
    "total_counts_df = pd.DataFrame(list(column_sums.items()), columns=['Name', 'Total'])\n",
    "#total_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted avg for NRELAP: 1.01\n",
      "Weighted avg for CESEV: 1.77\n"
     ]
    }
   ],
   "source": [
    "# Calculate the weighted sum for NRELAP\n",
    "nrelap_weighted_sum = 0\n",
    "total_count = 0\n",
    "for level_name, avg_score, _ in scores_with_std:\n",
    "    if level_name.startswith(\"NRELAP\"):\n",
    "        count = total_counts_df.loc[total_counts_df['Name'] == level_name, 'Total'].values[0]\n",
    "        nrelap_weighted_sum += avg_score * count\n",
    "        total_count += count\n",
    "nrelap_weighted_avg = nrelap_weighted_sum / total_count\n",
    "\n",
    "# Calculate the weighted sum for CESEV\n",
    "cesev_weighted_sum = 0\n",
    "total_count = 0\n",
    "for level_name, avg_score, _ in scores_with_std:\n",
    "    if level_name.startswith(\"CESEV\"):\n",
    "        count = total_counts_df.loc[total_counts_df['Name'] == level_name, 'Total'].values[0]\n",
    "        cesev_weighted_sum += avg_score * count\n",
    "        total_count += count\n",
    "cesev_weighted_avg = cesev_weighted_sum / total_count\n",
    "\n",
    "# Print the results\n",
    "print(f\"Weighted avg for NRELAP: {nrelap_weighted_avg:.2f}\")\n",
    "print(f\"Weighted avg for CESEV: {cesev_weighted_avg:.2f}\")\n",
    "\n",
    "cat_normalized_brier= [nrelap_weighted_avg, cesev_weighted_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Brier scores for each categorical variable:\n",
      "NRELAP: 1.03 (± 0.05)\n",
      "CESEV: 1.54 (± 0.51)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store scores\n",
    "scores_with_std = []\n",
    "variables_cat = yi_test_dummies_list[0].columns\n",
    "\n",
    "# Create a dictionary to store the scores for variables with the same letters before the '_'\n",
    "variable_scores_dict = {}\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for level_name in variables_cat: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Compute scores for the variable across all folds\n",
    "    for fold_index in range(len(yi_test_dummies_list)):\n",
    "        y_test = yi_test_dummies_list[fold_index][level_name] \n",
    "        y_prob = concatenated_dfs[fold_index][level_name] \n",
    "        y_prob_avg = yi_train_dummies_avg[fold_index][level_name] \n",
    "        \n",
    "        # Compute the Brier score and the normalized Brier score\n",
    "        brier_score = brier_score_loss(y_test, y_prob)\n",
    "        brier_baseline = brier_score_loss(y_test, y_prob_avg)\n",
    "        normalized_brier = brier_score / brier_baseline\n",
    "\n",
    "        # Append the normalized Brier score to the variable scores list\n",
    "        variable_scores.append(normalized_brier)\n",
    "    \n",
    "    # Check if the variable name has letters before the '_'\n",
    "    prefix = level_name.split('_')[0]\n",
    "    \n",
    "    # Add the normalized Brier scores to the dictionary based on the prefix\n",
    "    if prefix in variable_scores_dict:\n",
    "        variable_scores_dict[prefix].extend(variable_scores)\n",
    "    else:\n",
    "        variable_scores_dict[prefix] = variable_scores\n",
    "\n",
    "# Compute the average and standard deviation of normalized Brier score for each prefix\n",
    "for prefix, scores in variable_scores_dict.items():\n",
    "    avg_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    scores_with_std.append((prefix, avg_score, std_score))\n",
    "\n",
    "#cat_normalized_brier = []\n",
    "cat_std_brier = []\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "print(\"Normalized Brier scores for each categorical variable:\")\n",
    "for prefix, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{prefix}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    #cat_normalized_brier.append(avg_score)\n",
    "    cat_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_cv = []\n",
    "y_pred_cv = []\n",
    "\n",
    "for j in range(len(y_test_list)):  # 5\n",
    "    y_test_targ = []\n",
    "    y_pred_targ = []\n",
    "    nvar=y_test_list[0].shape[1]\n",
    "\n",
    "    for i in range(0, nvar):  # or (1, 5)\n",
    "        missing_rows_mask = y_test_list[j].iloc[:, i].isna()\n",
    "        y_test = y_test_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        y_pred = y_pred_list[j].iloc[:, i][~missing_rows_mask]\n",
    "        \n",
    "        y_test_targ.append(y_test)\n",
    "        y_pred_targ.append(y_pred)\n",
    "    \n",
    "    y_test_cv.append(y_test_targ)\n",
    "    y_pred_cv.append(y_pred_targ)\n",
    "# y_test_cv[fold][outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for each outcome (chain - true values):\n",
      "KFSS_M-2y: 0.71 (± 0.07)\n",
      "KFSS_P-2y: 0.81 (± 0.08)\n",
      "EDSS-2y: 0.50 (± 0.02)\n",
      "T25FW-2y: 0.99 (± 0.07)\n",
      "NHPT-2y: 0.93 (± 0.05)\n",
      "P_R36-SF12-after: 0.83 (± 0.03)\n",
      "M_R36-SF12-after: 1.02 (± 0.05)\n",
      "SES_after: 0.99 (± 0.04)\n",
      "SLEC_after: 1.05 (± 0.06)\n",
      "KFSS_M-after_2y: 0.71 (± 0.03)\n",
      "KFSS_P-after_2y: 1.00 (± 0.14)\n",
      "EDSS-after_2y: 0.57 (± 0.04)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store scores\n",
    "scores_with_std = []\n",
    "\n",
    "# Iterate over each outcome variable in the folds\n",
    "for variable_name in variables: \n",
    "    variable_scores = []\n",
    "    \n",
    "    # Check if the target variable is numerical or categorical\n",
    "    if y_test_cv[0][variables.index(variable_name)].dtype.kind in 'bifc':\n",
    "        # Compute scores for the variable across all folds\n",
    "        for fold_index in range(len(y_test_cv)):\n",
    "            y_test = y_test_cv[fold_index][variables.index(variable_name)] \n",
    "            y_pred = y_pred_cv[fold_index][variables.index(variable_name)] \n",
    "            y_train = y_train_list[fold_index][variable_name]\n",
    "\n",
    "            score = normalized_mean_squared_error(y_test, y_pred, y_train)\n",
    "            variable_scores.append(score)\n",
    "        \n",
    "        # Compute the average score for the variable across all folds\n",
    "        variable_avg_score = np.mean(variable_scores)\n",
    "        \n",
    "        # Compute the standard deviation for the variable across all folds\n",
    "        variable_std_score = np.std(variable_scores)\n",
    "        \n",
    "        # Append the tuple with three elements to the scores_with_std list\n",
    "        scores_with_std.append((variable_name, variable_avg_score, variable_std_score))\n",
    "\n",
    "num_normalized_brier=[]\n",
    "num_std_brier=[]\n",
    "# Print the scores with average and standard deviation along with variable names\n",
    "print(\"Scores for each outcome (chain - true values):\")\n",
    "for variable_name, avg_score, std_score in scores_with_std:\n",
    "    print(f\"{variable_name}: {avg_score:.2f} (± {std_score:.2f})\")\n",
    "    num_normalized_brier.append(avg_score)\n",
    "    num_std_brier.append(std_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7144871 , 0.80575847, 0.49564878, 0.99310597, 0.93164642,\n",
       "       0.83115637, 1.01895165, 0.99268779, 1.04569312, 0.71179869,\n",
       "       1.00387099, 0.57478256, 1.00778209, 1.77476551])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_normalized_brier = np.concatenate((num_normalized_brier, cat_normalized_brier))\n",
    "combined_normalized_brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07249766, 0.08231472, 0.02248682, 0.06899315, 0.05191767,\n",
       "       0.03324506, 0.04950392, 0.04363207, 0.06240683, 0.03327623,\n",
       "       0.14057758, 0.04182778, 0.05240342, 0.51275679])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_std_brier = np.concatenate((num_std_brier, cat_std_brier))\n",
    "combined_std_brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average normalized Brier score: 0.921581106947109\n"
     ]
    }
   ],
   "source": [
    "# Compute the average\n",
    "average_normalized_brier = np.mean(combined_normalized_brier)\n",
    "print(\"Average normalized Brier score:\", average_normalized_brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of the combined average: 0.5635881449558359\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have an array of individual values to average\n",
    "values_to_average = combined_normalized_brier  # Your individual values here\n",
    "\n",
    "# Assuming you have an array of standard deviations corresponding to the individual values\n",
    "individual_std_devs = combined_std_brier  # Your array of individual standard deviations here\n",
    "\n",
    "# Step 1: Calculate the combined average\n",
    "combined_average = np.mean(values_to_average)\n",
    "\n",
    "# Step 2: Calculate the standard error of the mean (SEM)\n",
    "sem = np.sqrt(np.sum(individual_std_devs**2) / len(values_to_average))\n",
    "\n",
    "# Step 3: Calculate the standard deviation of the combined average\n",
    "combined_std_dev = sem * np.sqrt(len(values_to_average))\n",
    "\n",
    "# Now you have the standard deviation of the combined average\n",
    "print(\"Standard deviation of the combined average:\", combined_std_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
